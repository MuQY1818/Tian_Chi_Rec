#!/usr/bin/env python3

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import os

def create_small_no_coldstart_dataset():
    """åˆ›å»ºä¸€ä¸ªå°çš„ã€æ— å†·å¯åŠ¨å•†å“çš„æ•°æ®é›†ç”¨äºæµ‹è¯•"""

    print("=== åˆ›å»ºå°æ•°æ®é›†ï¼ˆæ— å†·å¯åŠ¨å•†å“ï¼‰===")

    # è¯»å–åŸå§‹æ•°æ®çš„ä¸€ä¸ªå­é›†
    print("1. è¯»å–åŸå§‹æ•°æ®...")
    raw_file = "dataset/tianchi_fresh_comp_train_user_online_partB.txt"
    columns = ["user_id", "item_id", "behavior_type", "user_geohash", "item_category", "time"]

    # åªè¯»å–å‰100ä¸‡è¡Œä½œä¸ºåŸºç¡€
    df = pd.read_csv(raw_file, sep="\t", names=columns, nrows=1000000)
    print(f"åŸå§‹æ•°æ®: {len(df)} è¡Œ")

    # æ¸…ç†æ•°æ®
    df = df[df["behavior_type"].isin([1, 2, 3, 4])].copy()
    df["datetime"] = pd.to_datetime(df["time"], format="%Y-%m-%d %H", errors="coerce")
    df = df.dropna(subset=["user_id", "item_id", "datetime"]).reset_index(drop=True)

    # æ—¶é—´è¿‡æ»¤ï¼Œåªè¦12æœˆçš„æ•°æ®
    df = df[df["datetime"] >= "2014-12-01"].copy()
    print(f"12æœˆæ•°æ®: {len(df)} è¡Œ")

    # é€‰æ‹©æ´»è·ƒç”¨æˆ·å’Œå•†å“ï¼ˆè‡³å°‘æœ‰5æ¬¡äº¤äº’ï¼‰
    print("2. ç­›é€‰æ´»è·ƒç”¨æˆ·å’Œå•†å“...")
    user_counts = df.groupby("user_id").size()
    item_counts = df.groupby("item_id").size()

    active_users = user_counts[user_counts >= 5].index[:5000]  # é€‰æ‹©å‰5000æ´»è·ƒç”¨æˆ·
    active_items = item_counts[item_counts >= 5].index[:2000]  # é€‰æ‹©å‰2000æ´»è·ƒå•†å“

    # è¿‡æ»¤åˆ°æ´»è·ƒç”¨æˆ·å’Œå•†å“
    df = df[df["user_id"].isin(active_users) & df["item_id"].isin(active_items)].copy()
    print(f"æ´»è·ƒç”¨æˆ·å•†å“æ•°æ®: {len(df)} è¡Œ, {df['user_id'].nunique()} ç”¨æˆ·, {df['item_id'].nunique()} å•†å“")

    # ç¡®ä¿æ—¶é—´åˆ‡åˆ†åçš„æ•°æ®å®Œæ•´æ€§
    print("3. æ—¶é—´åˆ‡åˆ†...")
    train_cutoff = pd.to_datetime("2014-12-16 23:59:59")
    val_date = pd.to_datetime("2014-12-17")

    train_df = df[df["datetime"] <= train_cutoff].copy()
    val_df = df[df["datetime"].dt.date == val_date.date()].copy()

    # ç¡®ä¿éªŒè¯é›†ä¸­çš„å•†å“éƒ½åœ¨è®­ç»ƒé›†ä¸­å‡ºç°è¿‡
    train_items = set(train_df["item_id"].unique())
    val_df = val_df[val_df["item_id"].isin(train_items)].copy()

    # ç¡®ä¿éªŒè¯é›†ä¸­çš„ç”¨æˆ·éƒ½åœ¨è®­ç»ƒé›†ä¸­å‡ºç°è¿‡
    train_users = set(train_df["user_id"].unique())
    val_df = val_df[val_df["user_id"].isin(train_users)].copy()

    print(f"è®­ç»ƒæ•°æ®: {len(train_df)} è¡Œ, {train_df['user_id'].nunique()} ç”¨æˆ·, {train_df['item_id'].nunique()} å•†å“")
    print(f"éªŒè¯æ•°æ®: {len(val_df)} è¡Œ, {val_df['user_id'].nunique()} ç”¨æˆ·, {val_df['item_id'].nunique()} å•†å“")

    # éªŒè¯æ— å†·å¯åŠ¨
    val_items = set(val_df["item_id"].unique())
    cold_items = val_items - train_items
    print(f"å†·å¯åŠ¨å•†å“æ•°: {len(cold_items)} (åº”è¯¥ä¸º0)")

    if len(cold_items) > 0:
        print("âŒ ä»æœ‰å†·å¯åŠ¨å•†å“ï¼Œç»§ç»­è¿‡æ»¤...")
        val_df = val_df[~val_df["item_id"].isin(cold_items)]
        print(f"è¿‡æ»¤åéªŒè¯æ•°æ®: {len(val_df)} è¡Œ")

    # åˆå¹¶å¹¶ä¿å­˜
    print("4. ä¿å­˜å°æ•°æ®é›†...")
    small_df = pd.concat([train_df, val_df], ignore_index=True)
    small_df = small_df.sort_values(["user_id", "datetime"]).reset_index(drop=True)

    # ä¿å­˜åˆ°æ–°æ–‡ä»¶
    output_file = "dataset/small_dataset.txt"
    small_df.to_csv(output_file, sep="\t", header=False, index=False,
                   columns=["user_id", "item_id", "behavior_type", "user_geohash", "item_category", "time"])

    print(f"âœ… å°æ•°æ®é›†å·²ä¿å­˜: {output_file}")
    print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    print(f"   - æ€»è¡Œæ•°: {len(small_df):,}")
    print(f"   - ç”¨æˆ·æ•°: {small_df['user_id'].nunique():,}")
    print(f"   - å•†å“æ•°: {small_df['item_id'].nunique():,}")
    print(f"   - è®­ç»ƒè¡Œæ•°: {len(train_df):,}")
    print(f"   - éªŒè¯è¡Œæ•°: {len(val_df):,}")
    print(f"   - éªŒè¯ç”¨æˆ·æ•°: {val_df['user_id'].nunique():,}")

    return output_file

if __name__ == "__main__":
    create_small_no_coldstart_dataset()