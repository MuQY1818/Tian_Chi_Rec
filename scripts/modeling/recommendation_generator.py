#!/usr/bin/env python3
"""
æ¨èç”Ÿæˆå™¨
ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ç”Ÿæˆæœ€ç»ˆæ¨èåˆ—è¡¨
"""

import sys
import pandas as pd
import numpy as np
import joblib
import os
from tqdm import tqdm
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

sys.path.append('src')


class RecommendationGenerator:
    """æ¨èç”Ÿæˆå™¨"""

    def __init__(self, feature_dir="/mnt/data/tianchi_features",
                 model_dir="/mnt/data/tianchi_features",
                 output_dir="/mnt/data/tianchi_features"):
        self.feature_dir = feature_dir
        self.model_dir = model_dir
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)

        self.model = None
        self.feature_names = None
        self.user_features = None
        self.item_features = None

        print(f"ğŸ“ ç‰¹å¾ç›®å½•: {feature_dir}")
        print(f"ğŸ“ æ¨¡å‹ç›®å½•: {model_dir}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")

    def load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        print("ğŸ“‚ åŠ è½½è®­ç»ƒæ¨¡å‹...")

        model_file = os.path.join(self.model_dir, "lightgbm_model.pkl")
        feature_file = os.path.join(self.model_dir, "feature_names.pkl")
        metrics_file = os.path.join(self.model_dir, "model_metrics.pkl")

        if not all(os.path.exists(f) for f in [model_file, feature_file]):
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ç¼ºå¤±")
            return False

        self.model = joblib.load(model_file)
        self.feature_names = joblib.load(feature_file)

        if os.path.exists(metrics_file):
            self.metrics = joblib.load(metrics_file)
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (AUC: {self.metrics['auc']:.4f})")
        else:
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")

        return True

    def load_features(self):
        """åŠ è½½ç”¨æˆ·å’Œå•†å“ç‰¹å¾"""
        print("ğŸ“‚ åŠ è½½ç‰¹å¾æ•°æ®...")

        # åŠ è½½ç”¨æˆ·ç‰¹å¾
        user_feature_file = os.path.join(self.feature_dir, "user_features_cpp.csv")
        if os.path.exists(user_feature_file):
            self.user_features = pd.read_csv(user_feature_file)
            print(f"  ğŸ‘¥ ç”¨æˆ·ç‰¹å¾: {len(self.user_features):,} ç”¨æˆ·")
        else:
            print(f"  âŒ ç”¨æˆ·ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨")
            return False

        # åŠ è½½å•†å“ç‰¹å¾
        item_feature_file = os.path.join(self.feature_dir, "item_features.csv")
        if os.path.exists(item_feature_file):
            self.item_features = pd.read_csv(item_feature_file)
            print(f"  ğŸ“¦ å•†å“ç‰¹å¾: {len(self.item_features):,} å•†å“")
        else:
            print(f"  âŒ å•†å“ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨")
            return False

        # è½¬æ¢ä¸ºå­—å…¸ä¾¿äºæŸ¥æ‰¾
        self.user_feature_dict = self.user_features.set_index('user_id').to_dict('index')
        self.item_feature_dict = self.item_features.set_index('item_id').to_dict('index')

        return True

    def load_historical_interactions(self):
        """åŠ è½½å†å²äº¤äº’æ•°æ®ï¼ˆç”¨äºç”Ÿæˆäº¤äº’ç‰¹å¾ï¼‰"""
        print("ğŸ“Š åŠ è½½å†å²äº¤äº’æ•°æ®...")

        data_dir = "dataset/preprocess_16to18"
        data_files = [
            ("data_1216.txt", 16),
            ("data_1217.txt", 17),
            ("data_1218.txt", 18)
        ]

        columns = ["user_id", "item_id", "behavior_type", "user_geohash", "item_category", "time"]
        self.user_item_interactions = defaultdict(lambda: defaultdict(list))

        for filename, day in data_files:
            print(f"  ğŸ“… å¤„ç†ç¬¬{day}å·æ•°æ®...")

            file_path = os.path.join(data_dir, filename)
            chunk_size = 1000000

            for chunk in pd.read_csv(file_path, sep="\t", names=columns, chunksize=chunk_size):
                # åªå¤„ç†æœ‰ç‰¹å¾çš„ç”¨æˆ·å’Œå•†å“
                chunk = chunk[chunk['user_id'].isin(self.user_feature_dict.keys())]
                chunk = chunk[chunk['item_id'].isin(self.item_feature_dict.keys())]

                if len(chunk) == 0:
                    continue

                # è§£ææ—¶é—´
                chunk['datetime'] = pd.to_datetime(chunk['time'], format="%Y-%m-%d %H", errors="coerce")
                chunk = chunk.dropna(subset=['datetime'])

                # æå–äº¤äº’ä¿¡æ¯
                for _, row in chunk.iterrows():
                    user_id = int(row['user_id'])
                    item_id = int(row['item_id'])
                    behavior = int(row['behavior_type'])
                    timestamp = row['datetime']

                    self.user_item_interactions[user_id][item_id].append({
                        'behavior': behavior,
                        'timestamp': timestamp,
                        'day': day
                    })

        total_users = len(self.user_item_interactions)
        total_pairs = sum(len(items) for items in self.user_item_interactions.values())
        print(f"  âœ… äº¤äº’æ•°æ®åŠ è½½å®Œæˆ: {total_users:,} ç”¨æˆ·, {total_pairs:,} ç”¨æˆ·-å•†å“å¯¹")

    def generate_interaction_features(self, user_id, item_id):
        """ç”Ÿæˆç”¨æˆ·-å•†å“äº¤äº’ç‰¹å¾ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰"""
        interactions = self.user_item_interactions[user_id].get(item_id, [])

        # åŸºç¡€äº¤äº’ç‰¹å¾
        features = {
            'has_interaction': 1 if interactions else 0,
            'total_interactions': len(interactions),
            'browse_count': sum(1 for i in interactions if i['behavior'] == 1),
            'collect_count': sum(1 for i in interactions if i['behavior'] == 2),
            'cart_count': sum(1 for i in interactions if i['behavior'] == 3),
            'purchase_count': sum(1 for i in interactions if i['behavior'] == 4),
        }

        if interactions:
            # æ—¶é—´ç‰¹å¾
            timestamps = [i['timestamp'] for i in interactions]
            features['first_interaction_days_ago'] = (pd.Timestamp('2014-12-19') - min(timestamps)).days
            features['last_interaction_days_ago'] = (pd.Timestamp('2014-12-19') - max(timestamps)).days

            # æœ€è¿‘äº¤äº’
            recent_interactions = [i for i in interactions if i['day'] >= 17]  # æœ€è¿‘2å¤©
            features['recent_interactions'] = len(recent_interactions)
            features['recent_purchase_count'] = sum(1 for i in recent_interactions if i['behavior'] == 4)

            # è¡Œä¸ºè¿›å±•
            features['max_behavior_type'] = max(i['behavior'] for i in interactions)
            features['behavior_progression'] = features['max_behavior_type'] / 4.0

            # äº¤äº’é¢‘ç‡
            if len(set(i['day'] for i in interactions)) > 1:
                features['interaction_frequency'] = len(interactions) / len(set(i['day'] for i in interactions))
            else:
                features['interaction_frequency'] = len(interactions)

        else:
            # æ— äº¤äº’çš„é»˜è®¤å€¼
            features.update({
                'first_interaction_days_ago': 999,
                'last_interaction_days_ago': 999,
                'recent_interactions': 0,
                'recent_purchase_count': 0,
                'max_behavior_type': 0,
                'behavior_progression': 0,
                'interaction_frequency': 0,
            })

        return features

    def generate_candidate_items(self, user_id, top_k=100):
        """ä¸ºç”¨æˆ·ç”Ÿæˆå€™é€‰å•†å“"""
        # ç­–ç•¥1: ç”¨æˆ·å†å²äº¤äº’å•†å“
        historical_items = set()
        if user_id in self.user_item_interactions:
            historical_items = set(self.user_item_interactions[user_id].keys())

        # ç­–ç•¥2: çƒ­é—¨å•†å“
        popular_items = self.item_features.nlargest(50, 'total_interactions')['item_id'].tolist()

        # ç­–ç•¥3: ç”¨æˆ·ç±»åˆ«åå¥½å•†å“
        user_feature = self.user_feature_dict.get(user_id, {})
        preferred_category = user_feature.get('top_category', -1)

        category_items = []
        if preferred_category != -1:
            category_items = self.item_features[
                self.item_features['item_category'] == preferred_category
            ].nlargest(30, 'total_interactions')['item_id'].tolist()

        # åˆå¹¶å€™é€‰é›†
        candidates = list(historical_items) + popular_items + category_items
        candidates = list(set(candidates))  # å»é‡

        # ç¡®ä¿æœ‰è¶³å¤Ÿå€™é€‰å•†å“
        if len(candidates) < top_k:
            # è¡¥å……éšæœºå•†å“
            all_items = self.item_features['item_id'].tolist()
            additional_items = [item for item in all_items if item not in candidates]
            candidates.extend(additional_items[:top_k - len(candidates)])

        return candidates[:top_k]

    def predict_user_item_batch(self, user_id, item_ids, batch_size=1000):
        """æ‰¹é‡é¢„æµ‹ç”¨æˆ·å¯¹å•†å“çš„è´­ä¹°æ¦‚ç‡"""
        results = []

        # åˆ†æ‰¹å¤„ç†é¿å…å†…å­˜é—®é¢˜
        for i in range(0, len(item_ids), batch_size):
            batch_items = item_ids[i:i + batch_size]
            batch_features = []

            for item_id in batch_items:
                # æ„å»ºç‰¹å¾å‘é‡
                sample = {}

                # ç”¨æˆ·ç‰¹å¾
                user_features = self.user_feature_dict.get(user_id, {})
                for key, value in user_features.items():
                    sample[f'user_{key}'] = value

                # å•†å“ç‰¹å¾
                item_features = self.item_feature_dict.get(item_id, {})
                for key, value in item_features.items():
                    if key != 'item_id':
                        sample[f'item_{key}'] = value

                # äº¤äº’ç‰¹å¾
                interaction_features = self.generate_interaction_features(user_id, item_id)
                for key, value in interaction_features.items():
                    sample[f'interaction_{key}'] = value

                batch_features.append(sample)

            # è½¬æ¢ä¸ºDataFrameå¹¶ç¡®ä¿ç‰¹å¾é¡ºåº
            batch_df = pd.DataFrame(batch_features)

            # ç¡®ä¿æ‰€æœ‰ç‰¹å¾éƒ½å­˜åœ¨ï¼Œç¼ºå¤±çš„ç”¨0å¡«å……
            for feature in self.feature_names:
                if feature not in batch_df.columns:
                    batch_df[feature] = 0

            # æŒ‰è®­ç»ƒæ—¶çš„ç‰¹å¾é¡ºåºæ’åˆ—
            batch_df = batch_df[self.feature_names]

            # é¢„æµ‹
            probabilities = self.model.predict(batch_df, num_iteration=self.model.best_iteration)

            # ä¿å­˜ç»“æœ
            for j, prob in enumerate(probabilities):
                results.append({
                    'item_id': batch_items[j],
                    'probability': prob
                })

        return results

    def generate_recommendations_for_user(self, user_id, top_k=50):
        """ä¸ºå•ä¸ªç”¨æˆ·ç”Ÿæˆæ¨è"""
        # ç”Ÿæˆå€™é€‰å•†å“
        candidates = self.generate_candidate_items(user_id, top_k=100)

        # é¢„æµ‹è´­ä¹°æ¦‚ç‡
        predictions = self.predict_user_item_batch(user_id, candidates)

        # æ’åºå¹¶è¿”å›Top-K
        predictions.sort(key=lambda x: x['probability'], reverse=True)

        return predictions[:top_k]

    def generate_all_recommendations(self, top_k=50):
        """ä¸ºæ‰€æœ‰ç”¨æˆ·ç”Ÿæˆæ¨è"""
        print(f"\nğŸ¯ ç”Ÿæˆæ¨è (æ¯ç”¨æˆ·Top-{top_k})...")

        all_recommendations = []
        users = list(self.user_feature_dict.keys())

        print(f"  ğŸ‘¥ å¾…å¤„ç†ç”¨æˆ·æ•°: {len(users):,}")

        for user_id in tqdm(users, desc="ç”Ÿæˆæ¨è"):
            try:
                user_recs = self.generate_recommendations_for_user(user_id, top_k)

                for rec in user_recs:
                    all_recommendations.append({
                        'user_id': user_id,
                        'item_id': rec['item_id'],
                        'probability': rec['probability']
                    })

            except Exception as e:
                print(f"  âš ï¸  ç”¨æˆ· {user_id} æ¨èç”Ÿæˆå¤±è´¥: {e}")
                continue

        print(f"âœ… æ¨èç”Ÿæˆå®Œæˆ:")
        print(f"  ğŸ“Š æ€»æ¨èæ•°: {len(all_recommendations):,}")
        print(f"  ğŸ‘¥ ç”¨æˆ·æ•°: {len(set(r['user_id'] for r in all_recommendations)):,}")

        return all_recommendations

    def format_submission(self, recommendations):
        """æ ¼å¼åŒ–ä¸ºæäº¤æ ¼å¼"""
        print("\nğŸ“ æ ¼å¼åŒ–æäº¤æ–‡ä»¶...")

        # è½¬æ¢ä¸ºDataFrame
        rec_df = pd.DataFrame(recommendations)

        # æŒ‰æ¦‚ç‡æ’åº
        rec_df = rec_df.sort_values(['user_id', 'probability'], ascending=[True, False])

        # ç”Ÿæˆæœ€ç»ˆæäº¤æ ¼å¼ (user_id, item_id)
        submission = rec_df[['user_id', 'item_id']].copy()

        print(f"âœ… æäº¤æ ¼å¼åŒ–å®Œæˆ:")
        print(f"  ğŸ“Š æ¨èè®°å½•æ•°: {len(submission):,}")
        print(f"  ğŸ‘¥ ç”¨æˆ·æ•°: {submission['user_id'].nunique():,}")
        print(f"  ğŸ“¦ å•†å“æ•°: {submission['item_id'].nunique():,}")

        return submission

    def export_recommendations(self, recommendations):
        """å¯¼å‡ºæ¨èç»“æœ"""
        print("\nğŸ’¾ å¯¼å‡ºæ¨èç»“æœ...")

        # å¯¼å‡ºè¯¦ç»†æ¨èï¼ˆåŒ…å«æ¦‚ç‡ï¼‰
        detailed_file = os.path.join(self.output_dir, "detailed_recommendations.csv")
        pd.DataFrame(recommendations).to_csv(detailed_file, index=False)
        print(f"  ğŸ“Š è¯¦ç»†æ¨èå·²ä¿å­˜: {detailed_file}")

        # å¯¼å‡ºæäº¤æ ¼å¼ï¼ˆtabåˆ†éš”ï¼Œæ— è¡¨å¤´ï¼‰
        submission = self.format_submission(recommendations)
        submission_file = os.path.join(self.output_dir, "final_submission.txt")
        submission.to_csv(submission_file, index=False, header=False, sep='\t')
        print(f"  ğŸ“ æäº¤æ–‡ä»¶å·²ä¿å­˜: {submission_file}")

        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total_recommendations': len(recommendations),
            'unique_users': len(set(r['user_id'] for r in recommendations)),
            'unique_items': len(set(r['item_id'] for r in recommendations)),
            'avg_probability': np.mean([r['probability'] for r in recommendations]),
            'min_probability': min(r['probability'] for r in recommendations),
            'max_probability': max(r['probability'] for r in recommendations)
        }

        stats_file = os.path.join(self.output_dir, "recommendation_stats.txt")
        with open(stats_file, 'w') as f:
            f.write("=== æ¨èç»Ÿè®¡ä¿¡æ¯ ===\n")
            for key, value in stats.items():
                f.write(f"{key}: {value}\n")
        print(f"  ğŸ“Š ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: {stats_file}")

        print(f"âœ… æ¨èå¯¼å‡ºå®Œæˆ!")
        return submission_file

    def analyze_recommendations(self, recommendations):
        """åˆ†ææ¨èç»“æœ"""
        print("\nğŸ” æ¨èç»“æœåˆ†æ...")

        rec_df = pd.DataFrame(recommendations)

        # æ¦‚ç‡åˆ†å¸ƒåˆ†æ
        print(f"ğŸ“Š æ¦‚ç‡åˆ†å¸ƒ:")
        print(f"  æœ€é«˜æ¦‚ç‡: {rec_df['probability'].max():.4f}")
        print(f"  æœ€ä½æ¦‚ç‡: {rec_df['probability'].min():.4f}")
        print(f"  å¹³å‡æ¦‚ç‡: {rec_df['probability'].mean():.4f}")
        print(f"  æ¦‚ç‡æ ‡å‡†å·®: {rec_df['probability'].std():.4f}")

        # ç”¨æˆ·æ¨èæ•°åˆ†æ
        user_rec_counts = rec_df.groupby('user_id').size()
        print(f"\nğŸ‘¥ ç”¨æˆ·æ¨èæ•°åˆ†æ:")
        print(f"  å¹³å‡æ¯ç”¨æˆ·æ¨èæ•°: {user_rec_counts.mean():.1f}")
        print(f"  æœ€å¤šæ¨èæ•°: {user_rec_counts.max()}")
        print(f"  æœ€å°‘æ¨èæ•°: {user_rec_counts.min()}")

        # å•†å“æ¨èæ¬¡æ•°åˆ†æ
        item_rec_counts = rec_df.groupby('item_id').size()
        print(f"\nğŸ“¦ å•†å“æ¨èæ¬¡æ•°åˆ†æ:")
        print(f"  è¢«æ¨èå•†å“æ•°: {len(item_rec_counts):,}")
        print(f"  å¹³å‡è¢«æ¨èæ¬¡æ•°: {item_rec_counts.mean():.1f}")
        print(f"  æœ€å¤šè¢«æ¨èæ¬¡æ•°: {item_rec_counts.max()}")

        # Topæ¨èå•†å“
        top_items = item_rec_counts.nlargest(10)
        print(f"\nğŸ”¥ æœ€çƒ­é—¨æ¨èå•†å“:")
        for item_id, count in top_items.items():
            print(f"  å•†å“ {item_id}: è¢«æ¨è {count} æ¬¡")


def main():
    """ä¸»å‡½æ•°"""
    print("=== æ¨èç”Ÿæˆå™¨ ===")
    print("ğŸ¯ ç›®æ ‡ï¼šç”Ÿæˆæœ€ç»ˆæ¨èåˆ—è¡¨")

    generator = RecommendationGenerator()

    # 1. åŠ è½½æ¨¡å‹
    if not generator.load_model():
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥")
        return

    # 2. åŠ è½½ç‰¹å¾
    if not generator.load_features():
        print("âŒ ç‰¹å¾åŠ è½½å¤±è´¥")
        return

    # 3. åŠ è½½å†å²äº¤äº’
    generator.load_historical_interactions()

    # 4. ç”Ÿæˆæ¨è
    recommendations = generator.generate_all_recommendations(top_k=50)

    # 5. åˆ†ææ¨èç»“æœ
    generator.analyze_recommendations(recommendations)

    # 6. å¯¼å‡ºæ¨è
    submission_file = generator.export_recommendations(recommendations)

    print(f"\nğŸ‰ æ¨èç”Ÿæˆå®Œæˆ!")
    print(f"ğŸ“ æäº¤æ–‡ä»¶: {submission_file}")
    print(f"ğŸ† å¯ç”¨äºæ¯”èµ›æäº¤!")


if __name__ == "__main__":
    main()