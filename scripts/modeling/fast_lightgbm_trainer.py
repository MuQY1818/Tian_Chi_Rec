#!/usr/bin/env python3
"""
å¿«é€ŸLightGBMè®­ç»ƒå™¨
é’ˆå¯¹ç°æœ‰39ç»´ç”¨æˆ·ç‰¹å¾ä¼˜åŒ–çš„è®­ç»ƒå™¨
"""

import sys
import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, precision_recall_curve, f1_score
import joblib
import os
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

sys.path.append('src')


class FastLightGBMTrainer:
    """å¿«é€ŸLightGBMè®­ç»ƒå™¨"""

    def __init__(self, feature_dir="/mnt/data/tianchi_features"):
        self.feature_dir = feature_dir
        os.makedirs(feature_dir, exist_ok=True)

        self.model = None
        self.feature_names = None
        self.best_threshold = 0.5

        print(f"ğŸ“ ç‰¹å¾ç›®å½•: {feature_dir}")

    def load_training_data(self):
        """åŠ è½½å¿«é€Ÿç”Ÿæˆçš„è®­ç»ƒæ•°æ®"""
        print("ğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®...")

        train_file = os.path.join(self.feature_dir, "fast_training_samples.csv")
        if not os.path.exists(train_file):
            print(f"âŒ è®­ç»ƒæ–‡ä»¶ä¸å­˜åœ¨: {train_file}")
            print("è¯·å…ˆè¿è¡Œ: python scripts/feature_engineering/fast_sample_generator.py")
            return None

        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(train_file) / (1024**2)
        print(f"  ğŸ“ æ–‡ä»¶å¤§å°: {file_size:.1f} MB")

        # åŠ è½½æ•°æ®
        df = pd.read_csv(train_file)

        print(f"âœ… è®­ç»ƒæ•°æ®åŠ è½½å®Œæˆ:")
        print(f"  ğŸ“ æ ·æœ¬æ•°: {len(df):,}")
        print(f"  ğŸ”§ ç‰¹å¾æ•°: {len(df.columns)-3}")
        print(f"  ğŸ¯ æ­£æ ·æœ¬æ¯”ä¾‹: {df['label'].mean():.3f}")

        return df

    def prepare_features(self, df):
        """å‡†å¤‡ç‰¹å¾æ•°æ®ï¼ˆå¿«é€Ÿç‰ˆæœ¬ï¼‰"""
        print("\nğŸ”§ å‡†å¤‡ç‰¹å¾æ•°æ®...")

        # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
        feature_cols = [col for col in df.columns if col not in ['user_id', 'item_id', 'label']]
        X = df[feature_cols]
        y = df['label']

        self.feature_names = feature_cols

        print(f"  ğŸ”§ ç‰¹å¾ç»´åº¦: {X.shape}")
        print(f"  ğŸ¯ æ ‡ç­¾åˆ†å¸ƒ: {y.value_counts().to_dict()}")

        # å¿«é€Ÿæ•°æ®æ¸…ç†
        missing_count = X.isnull().sum().sum()
        if missing_count > 0:
            print(f"  ğŸ”§ å¡«å…… {missing_count} ä¸ªç¼ºå¤±å€¼")
            X = X.fillna(0)

        # æ£€æŸ¥æ— ç©·å€¼
        inf_count = np.isinf(X.select_dtypes(include=[np.number])).sum().sum()
        if inf_count > 0:
            print(f"  ğŸ”§ æ›¿æ¢ {inf_count} ä¸ªæ— ç©·å€¼")
            X = X.replace([np.inf, -np.inf], 0)

        print(f"  âœ… æ•°æ®å‡†å¤‡å®Œæˆ")

        return X, y

    def train_fast_model(self, X, y, validation_split=0.2):
        """å¿«é€Ÿè®­ç»ƒæ¨¡å‹"""
        print(f"\nğŸš€ å¼€å§‹å¿«é€Ÿè®­ç»ƒ...")

        # åˆ’åˆ†æ•°æ®
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=validation_split, random_state=42, stratify=y
        )

        print(f"  ğŸ“Š æ•°æ®åˆ’åˆ†:")
        print(f"    è®­ç»ƒé›†: {len(X_train):,}, æ­£æ ·æœ¬ç‡: {y_train.mean():.3f}")
        print(f"    éªŒè¯é›†: {len(X_val):,}, æ­£æ ·æœ¬ç‡: {y_val.mean():.3f}")

        # ä¼˜åŒ–çš„LightGBMå‚æ•°ï¼ˆé’ˆå¯¹å¿«é€Ÿè®­ç»ƒï¼‰
        params = {
            'objective': 'binary',
            'metric': 'auc',
            'boosting_type': 'gbdt',
            'num_leaves': 63,  # å‡å°‘å¶å­æ•°åŠ é€Ÿ
            'learning_rate': 0.1,  # æé«˜å­¦ä¹ ç‡
            'feature_fraction': 0.8,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'min_child_samples': 10,  # å‡å°‘æœ€å°æ ·æœ¬æ•°
            'verbosity': -1,
            'random_state': 42,
            'n_jobs': -1
        }

        print(f"ğŸ“‹ å¿«é€Ÿè®­ç»ƒå‚æ•°:")
        key_params = ['num_leaves', 'learning_rate', 'min_child_samples']
        for key in key_params:
            print(f"  {key}: {params[key]}")

        # å‡†å¤‡æ•°æ®é›†
        train_data = lgb.Dataset(X_train, label=y_train, feature_name=self.feature_names)
        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)

        # è®­ç»ƒï¼ˆå¿«é€Ÿç‰ˆæœ¬ï¼šå‡å°‘è½®æ•°ï¼‰
        print(f"\nğŸ”„ å¼€å§‹è®­ç»ƒ...")
        self.model = lgb.train(
            params=params,
            train_set=train_data,
            valid_sets=[train_data, val_data],
            valid_names=['train', 'eval'],
            num_boost_round=300,  # å‡å°‘è®­ç»ƒè½®æ•°
            callbacks=[
                lgb.early_stopping(stopping_rounds=30),  # æ›´æ—©åœæ­¢
                lgb.log_evaluation(period=50)
            ]
        )

        print(f"âœ… å¿«é€Ÿè®­ç»ƒå®Œæˆ!")
        print(f"  ğŸŒ³ å®é™…è½®æ•°: {self.model.best_iteration}")

        # å¿«é€Ÿè¯„ä¼°
        metrics = self._evaluate_model(X_val, y_val)

        return metrics

    def _evaluate_model(self, X_val, y_val):
        """å¿«é€Ÿæ¨¡å‹è¯„ä¼°"""
        print("\nğŸ“Š æ¨¡å‹è¯„ä¼°...")

        # é¢„æµ‹
        y_pred_proba = self.model.predict(X_val, num_iteration=self.model.best_iteration)

        # AUC
        auc = roc_auc_score(y_val, y_pred_proba)
        print(f"  ğŸ“ˆ AUC: {auc:.4f}")

        # æ‰¾æœ€ä½³é˜ˆå€¼
        precision, recall, thresholds = precision_recall_curve(y_val, y_pred_proba)
        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)
        best_idx = np.argmax(f1_scores)
        self.best_threshold = thresholds[best_idx]
        best_f1 = f1_scores[best_idx]

        print(f"  ğŸ¯ æœ€ä½³é˜ˆå€¼: {self.best_threshold:.4f}")
        print(f"  ğŸ“Š æœ€ä½³F1: {best_f1:.4f}")

        return {
            'auc': auc,
            'best_threshold': self.best_threshold,
            'best_f1': best_f1
        }

    def analyze_top_features(self, top_n=15):
        """åˆ†æTopç‰¹å¾é‡è¦æ€§"""
        print(f"\nğŸ” Top {top_n} é‡è¦ç‰¹å¾:")

        importance = self.model.feature_importance(importance_type='gain')
        feature_importance = pd.DataFrame({
            'feature': self.feature_names,
            'importance': importance
        }).sort_values('importance', ascending=False)

        for i, row in feature_importance.head(top_n).iterrows():
            feature_type = "ğŸ‘¥" if row['feature'].startswith('user_') else \
                          "ğŸ“¦" if row['feature'].startswith('item_') else "ğŸ”—"
            print(f"  {feature_type} {row['feature']}: {row['importance']:.1f}")

        return feature_importance

    def save_fast_model(self, metrics):
        """ä¿å­˜å¿«é€Ÿæ¨¡å‹"""
        print("\nğŸ’¾ ä¿å­˜æ¨¡å‹...")

        # ä¿å­˜æ¨¡å‹
        model_file = os.path.join(self.feature_dir, "fast_lightgbm_model.pkl")
        joblib.dump(self.model, model_file)

        # ä¿å­˜é…ç½®
        config = {
            'feature_names': self.feature_names,
            'best_threshold': self.best_threshold,
            'metrics': metrics,
            'model_type': 'Fast_LightGBM'
        }

        config_file = os.path.join(self.feature_dir, "fast_model_config.pkl")
        joblib.dump(config, config_file)

        print(f"  ğŸ“¦ æ¨¡å‹å·²ä¿å­˜: {model_file}")
        print(f"  âš™ï¸  é…ç½®å·²ä¿å­˜: {config_file}")

    def load_fast_model(self):
        """åŠ è½½å¿«é€Ÿæ¨¡å‹"""
        model_file = os.path.join(self.feature_dir, "fast_lightgbm_model.pkl")
        config_file = os.path.join(self.feature_dir, "fast_model_config.pkl")

        if os.path.exists(model_file) and os.path.exists(config_file):
            self.model = joblib.load(model_file)
            config = joblib.load(config_file)

            self.feature_names = config['feature_names']
            self.best_threshold = config['best_threshold']

            print(f"âœ… å¿«é€Ÿæ¨¡å‹åŠ è½½æˆåŠŸ")
            print(f"  ğŸ“Š AUC: {config['metrics']['auc']:.4f}")
            return True
        else:
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
            return False

    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡"""
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒæˆ–åŠ è½½")

        return self.model.predict(X, num_iteration=self.model.best_iteration)


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– === å¿«é€ŸLightGBMè®­ç»ƒå™¨ === ğŸ¤–")
    print("ğŸ¯ ç›®æ ‡ï¼šåŸºäºç°æœ‰39ç»´ç”¨æˆ·ç‰¹å¾å¿«é€Ÿè®­ç»ƒæ¨¡å‹")
    print("âš¡ é¢„è®¡è€—æ—¶ï¼š2-3åˆ†é’Ÿ")
    print("â”" * 50)

    trainer = FastLightGBMTrainer()

    # 1. åŠ è½½è®­ç»ƒæ•°æ®
    df = trainer.load_training_data()
    if df is None:
        return

    # 2. å‡†å¤‡ç‰¹å¾
    X, y = trainer.prepare_features(df)

    # 3. å¿«é€Ÿè®­ç»ƒ
    metrics = trainer.train_fast_model(X, y)

    # 4. ç‰¹å¾é‡è¦æ€§
    feature_importance = trainer.analyze_top_features()

    # 5. ä¿å­˜æ¨¡å‹
    trainer.save_fast_model(metrics)

    print(f"\nğŸ‰ å¿«é€Ÿè®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“ˆ AUC: {metrics['auc']:.4f}")
    print(f"ğŸ“Š F1: {metrics['best_f1']:.4f}")
    print(f"âš¡ è®­ç»ƒé€Ÿåº¦: æ¯”å®Œæ•´ç‰ˆæœ¬å¿«3-5å€")
    print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {trainer.feature_dir}")


if __name__ == "__main__":
    main()