#!/usr/bin/env python3
"""
å•†å“ç‰¹å¾æå–å™¨
ä»16-18å·æ•°æ®ä¸­æå–å•†å“ç‰¹å¾
"""

import sys
import pandas as pd
import numpy as np
from collections import defaultdict, Counter
import os
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

sys.path.append('src')


class ItemFeatureExtractor:
    """å•†å“ç‰¹å¾æå–å™¨"""

    def __init__(self, data_dir="dataset/preprocess_16to18", output_dir="/mnt/data/tianchi_features"):
        self.data_dir = data_dir
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)

        # æ•°æ®æ–‡ä»¶
        self.data_files = [
            "data_1216.txt",
            "data_1217.txt",
            "data_1218.txt"
        ]

        # å•†å“ç»Ÿè®¡
        self.item_stats = defaultdict(lambda: {
            # åŸºç¡€ç»Ÿè®¡
            'total_interactions': 0,
            'browse_count': 0,
            'collect_count': 0,
            'cart_count': 0,
            'purchase_count': 0,
            'unique_users': set(),

            # æ—¶é—´åˆ†å¸ƒ
            'day_interactions': defaultdict(int),  # æŒ‰æ—¥æœŸç»Ÿè®¡
            'hour_interactions': defaultdict(int), # æŒ‰å°æ—¶ç»Ÿè®¡

            # åœ°ç†åˆ†å¸ƒ
            'geo_interactions': defaultdict(int),

            # æ—¶é—´åºåˆ—
            'interaction_times': [],
        })

    def load_item_catalog(self):
        """åŠ è½½å•†å“ç›®å½•(På­é›†)"""
        print("ğŸ“š åŠ è½½å•†å“å­é›†P...")

        item_file = "dataset/tianchi_fresh_comp_train_item_online.txt"
        columns = ["item_id", "item_geohash", "item_category"]

        item_df = pd.read_csv(item_file, sep="\t", names=columns)
        # å¤„ç†ç©ºçš„geohash
        item_df['item_geohash'] = item_df['item_geohash'].fillna('unknown')

        print(f"  ğŸ“¦ På­é›†å•†å“æ•°: {len(item_df):,}")

        # è½¬æ¢ä¸ºå­—å…¸ä¾¿äºæŸ¥æ‰¾
        self.item_catalog = dict(zip(item_df['item_id'], item_df['item_category']))
        print(f"  ğŸ·ï¸  å•†å“ç±»åˆ«æ•°: {item_df['item_category'].nunique()}")

        return item_df

    def process_data_files(self):
        """å¤„ç†16-18å·æ•°æ®æ–‡ä»¶"""
        print("\nğŸ“‚ å¤„ç†é¢„å¤„ç†æ•°æ®æ–‡ä»¶...")

        columns = ["user_id", "item_id", "behavior_type", "user_geohash", "item_category", "time"]

        for day, filename in enumerate(self.data_files, 16):
            print(f"\nğŸ“… å¤„ç†ç¬¬{day}å·æ•°æ®: {filename}")

            file_path = os.path.join(self.data_dir, filename)

            # åˆ†å—è¯»å–å¤§æ–‡ä»¶
            chunk_size = 1000000
            chunk_count = 0

            for chunk in pd.read_csv(file_path, sep="\t", names=columns, chunksize=chunk_size):
                chunk_count += 1

                print(f"  ğŸ”„ å¤„ç†å— {chunk_count} (è¡Œæ•°: {len(chunk):,})")

                # åªå¤„ç†På­é›†å•†å“
                chunk = chunk[chunk['item_id'].isin(self.item_catalog.keys())]

                if len(chunk) == 0:
                    continue

                # è§£ææ—¶é—´
                chunk['datetime'] = pd.to_datetime(chunk['time'], format="%Y-%m-%d %H", errors="coerce")
                chunk = chunk.dropna(subset=['datetime'])

                # æ·»åŠ æ—¶é—´ç‰¹å¾
                chunk['date'] = chunk['datetime'].dt.date
                chunk['hour'] = chunk['datetime'].dt.hour

                # æ›´æ–°å•†å“ç»Ÿè®¡
                for _, row in tqdm(chunk.iterrows(),
                                 total=len(chunk),
                                 desc=f"    æ›´æ–°ç»Ÿè®¡",
                                 leave=False,
                                 miniters=len(chunk)//10):
                    self._update_item_stats(row, day)

            print(f"  âœ… ç¬¬{day}å·æ•°æ®å¤„ç†å®Œæˆ")

        print(f"\nğŸ“Š ç»Ÿè®¡æ±‡æ€»:")
        print(f"  ğŸ“¦ å¤„ç†å•†å“æ•°: {len(self.item_stats):,}")
        total_interactions = sum(stats['total_interactions'] for stats in self.item_stats.values())
        print(f"  ğŸ“ˆ æ€»äº¤äº’æ•°: {total_interactions:,}")

    def _update_item_stats(self, row, day):
        """æ›´æ–°å•ä¸ªå•†å“çš„ç»Ÿè®¡ä¿¡æ¯"""
        item_id = int(row['item_id'])
        user_id = int(row['user_id'])
        behavior = int(row['behavior_type'])
        geo = row['user_geohash'] if pd.notna(row['user_geohash']) else "unknown"
        hour = int(row['hour'])
        date = row['date']

        stats = self.item_stats[item_id]

        # åŸºç¡€ç»Ÿè®¡
        stats['total_interactions'] += 1
        stats['unique_users'].add(user_id)

        if behavior == 1:
            stats['browse_count'] += 1
        elif behavior == 2:
            stats['collect_count'] += 1
        elif behavior == 3:
            stats['cart_count'] += 1
        elif behavior == 4:
            stats['purchase_count'] += 1

        # æ—¶é—´åˆ†å¸ƒ
        stats['day_interactions'][day] += 1
        stats['hour_interactions'][hour] += 1

        # åœ°ç†åˆ†å¸ƒ
        stats['geo_interactions'][geo] += 1

        # æ—¶é—´åºåˆ—
        stats['interaction_times'].append(row['datetime'])

    def generate_item_features(self):
        """ç”Ÿæˆå•†å“ç‰¹å¾"""
        print("\nğŸ”§ ç”Ÿæˆå•†å“ç‰¹å¾...")

        features_list = []

        for item_id, stats in tqdm(self.item_stats.items(), desc="ç”Ÿæˆç‰¹å¾"):
            features = {
                'item_id': item_id,
                'item_category': self.item_catalog.get(item_id, -1),

                # åŸºç¡€æµè¡Œåº¦ç‰¹å¾
                'total_interactions': stats['total_interactions'],
                'unique_users_count': len(stats['unique_users']),
                'browse_count': stats['browse_count'],
                'collect_count': stats['collect_count'],
                'cart_count': stats['cart_count'],
                'purchase_count': stats['purchase_count'],

                # è½¬åŒ–ç‡ç‰¹å¾
                'collect_rate': stats['collect_count'] / max(stats['browse_count'], 1),
                'cart_rate': stats['cart_count'] / max(stats['browse_count'], 1),
                'purchase_rate': stats['purchase_count'] / max(stats['browse_count'], 1),
                'buy_conversion': stats['purchase_count'] / max(stats['total_interactions'], 1),

                # ç”¨æˆ·å¤šæ ·æ€§
                'user_interaction_avg': stats['total_interactions'] / max(len(stats['unique_users']), 1),

                # æ—¶é—´è¶‹åŠ¿ç‰¹å¾
                'day16_interactions': stats['day_interactions'].get(16, 0),
                'day17_interactions': stats['day_interactions'].get(17, 0),
                'day18_interactions': stats['day_interactions'].get(18, 0),

                # æ—¶é—´æ¨¡å¼
                'morning_rate': sum(stats['hour_interactions'][h] for h in range(6, 12)) / max(stats['total_interactions'], 1),
                'afternoon_rate': sum(stats['hour_interactions'][h] for h in range(12, 18)) / max(stats['total_interactions'], 1),
                'evening_rate': sum(stats['hour_interactions'][h] for h in range(18, 24)) / max(stats['total_interactions'], 1),
                'night_rate': sum(stats['hour_interactions'][h] for h in range(0, 6)) / max(stats['total_interactions'], 1),

                # åœ°ç†ç‰¹å¾
                'unique_geo_count': len(stats['geo_interactions']),
                'geo_concentration': max(stats['geo_interactions'].values()) / max(stats['total_interactions'], 1) if stats['geo_interactions'] else 0,
            }

            # è®¡ç®—è¶‹åŠ¿ç‰¹å¾
            day_counts = [stats['day_interactions'].get(d, 0) for d in [16, 17, 18]]
            features['trend_slope'] = self._calculate_trend(day_counts)
            features['trend_volatility'] = np.std(day_counts) if len(day_counts) > 1 else 0

            # æ´»è·ƒå°æ—¶æ•°
            features['active_hours_count'] = sum(1 for count in stats['hour_interactions'].values() if count > 0)

            features_list.append(features)

        return pd.DataFrame(features_list)

    def _calculate_trend(self, day_counts):
        """è®¡ç®—è¶‹åŠ¿æ–œç‡"""
        if len(day_counts) < 2:
            return 0

        x = np.arange(len(day_counts))
        y = np.array(day_counts)

        # ç®€å•çº¿æ€§å›å½’æ–œç‡
        if np.sum((x - np.mean(x))**2) == 0:
            return 0

        slope = np.sum((x - np.mean(x)) * (y - np.mean(y))) / np.sum((x - np.mean(x))**2)
        return slope

    def calculate_category_features(self, item_df):
        """è®¡ç®—ç±»åˆ«çº§åˆ«ç‰¹å¾"""
        print("ğŸ·ï¸ è®¡ç®—ç±»åˆ«ç‰¹å¾...")

        category_stats = defaultdict(lambda: {
            'total_interactions': 0,
            'total_purchases': 0,
            'item_count': 0
        })

        # èšåˆç±»åˆ«ç»Ÿè®¡
        for item_id, stats in self.item_stats.items():
            category = self.item_catalog.get(item_id, -1)
            if category != -1:
                category_stats[category]['total_interactions'] += stats['total_interactions']
                category_stats[category]['total_purchases'] += stats['purchase_count']
                category_stats[category]['item_count'] += 1

        # ä¸ºæ¯ä¸ªå•†å“æ·»åŠ ç±»åˆ«ç‰¹å¾
        for features in item_df:
            category = features['item_category']
            if category in category_stats:
                cat_stats = category_stats[category]
                features['category_popularity'] = cat_stats['total_interactions']
                features['category_purchase_rate'] = cat_stats['total_purchases'] / max(cat_stats['total_interactions'], 1)
                features['category_competition'] = cat_stats['item_count']  # ç±»åˆ«å†…å•†å“ç«äº‰åº¦
            else:
                features['category_popularity'] = 0
                features['category_purchase_rate'] = 0
                features['category_competition'] = 1

        return item_df

    def export_features(self):
        """å¯¼å‡ºå•†å“ç‰¹å¾"""
        print("\nğŸ’¾ å¯¼å‡ºå•†å“ç‰¹å¾...")

        # ç”Ÿæˆç‰¹å¾
        item_features_df = self.generate_item_features()

        # æ·»åŠ ç±»åˆ«ç‰¹å¾
        features_list = item_features_df.to_dict('records')
        features_list = self.calculate_category_features(features_list)
        item_features_df = pd.DataFrame(features_list)

        # ä¿å­˜ç‰¹å¾
        output_file = os.path.join(self.output_dir, "item_features.csv")
        item_features_df.to_csv(output_file, index=False)

        print(f"âœ… å•†å“ç‰¹å¾å·²ä¿å­˜: {output_file}")
        print(f"  ğŸ“¦ å•†å“æ•°: {len(item_features_df):,}")
        print(f"  ğŸ”§ ç‰¹å¾æ•°: {len(item_features_df.columns)-1}")

        # æ˜¾ç¤ºç‰¹å¾ç¤ºä¾‹
        print(f"\nğŸ“‹ ç‰¹å¾ç¤ºä¾‹:")
        sample_features = ['item_id', 'total_interactions', 'purchase_count', 'purchase_rate', 'trend_slope']
        print(item_features_df[sample_features].head())

        # ç‰¹å¾ç»Ÿè®¡
        print(f"\nğŸ“Š ç‰¹å¾ç»Ÿè®¡:")
        print(f"  ğŸ”¥ æœ€çƒ­é—¨å•†å“äº¤äº’æ•°: {item_features_df['total_interactions'].max():,}")
        print(f"  ğŸ’° æœ€é«˜è´­ä¹°ç‡: {item_features_df['purchase_rate'].max():.3f}")
        print(f"  ğŸ“ˆ å¹³å‡è¶‹åŠ¿æ–œç‡: {item_features_df['trend_slope'].mean():.2f}")

        return item_features_df


def main():
    """ä¸»å‡½æ•°"""
    print("=== å•†å“ç‰¹å¾æå–å™¨ ===")
    print("ğŸ¯ ç›®æ ‡ï¼šä»16-18å·æ•°æ®æå–å•†å“ç‰¹å¾")

    extractor = ItemFeatureExtractor()

    # 1. åŠ è½½å•†å“ç›®å½•
    item_catalog_df = extractor.load_item_catalog()

    # 2. å¤„ç†æ•°æ®æ–‡ä»¶
    extractor.process_data_files()

    # 3. ç”Ÿæˆå¹¶å¯¼å‡ºç‰¹å¾
    item_features_df = extractor.export_features()

    print(f"\nğŸ‰ å•†å“ç‰¹å¾æå–å®Œæˆ!")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {extractor.output_dir}")
    print(f"ä¸‹ä¸€æ­¥ï¼šç”Ÿæˆç”¨æˆ·-å•†å“äº¤äº’è®­ç»ƒæ ·æœ¬")


if __name__ == "__main__":
    main()