#!/usr/bin/env python3
"""
è®­ç»ƒæ ·æœ¬ç”Ÿæˆå™¨
æ„å»ºç”¨æˆ·-å•†å“äº¤äº’é¢„æµ‹çš„è®­ç»ƒæ ·æœ¬
"""

import sys
import pandas as pd
import numpy as np
from collections import defaultdict
import os
from tqdm import tqdm
import random
import warnings
warnings.filterwarnings('ignore')

sys.path.append('src')


class TrainingSampleGenerator:
    """è®­ç»ƒæ ·æœ¬ç”Ÿæˆå™¨"""

    def __init__(self, data_dir="dataset/preprocess_16to18",
                 feature_dir="/mnt/data/tianchi_features",
                 output_dir="/mnt/data/tianchi_features"):
        self.data_dir = data_dir
        self.feature_dir = feature_dir
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)

        self.user_features = None
        self.item_features = None
        self.user_item_interactions = defaultdict(lambda: defaultdict(list))

        print(f"ğŸ“ æ•°æ®ç›®å½•: {data_dir}")
        print(f"ğŸ“ ç‰¹å¾ç›®å½•: {feature_dir}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")

    def load_features(self):
        """åŠ è½½ç”¨æˆ·å’Œå•†å“ç‰¹å¾"""
        print("ğŸ“‚ åŠ è½½ç‰¹å¾æ•°æ®...")

        # åŠ è½½ç”¨æˆ·ç‰¹å¾
        user_feature_file = os.path.join(self.feature_dir, "user_features_cpp.csv")
        if os.path.exists(user_feature_file):
            self.user_features = pd.read_csv(user_feature_file)
            print(f"  ğŸ‘¥ ç”¨æˆ·ç‰¹å¾: {len(self.user_features):,} ç”¨æˆ·, {len(self.user_features.columns)-1} ç»´ç‰¹å¾")
        else:
            print(f"  âŒ æœªæ‰¾åˆ°ç”¨æˆ·ç‰¹å¾æ–‡ä»¶: {user_feature_file}")
            return False

        # åŠ è½½å•†å“ç‰¹å¾
        item_feature_file = os.path.join(self.feature_dir, "item_features.csv")
        if os.path.exists(item_feature_file):
            self.item_features = pd.read_csv(item_feature_file)
            print(f"  ğŸ“¦ å•†å“ç‰¹å¾: {len(self.item_features):,} å•†å“, {len(self.item_features.columns)-1} ç»´ç‰¹å¾")
        else:
            print(f"  âŒ æœªæ‰¾åˆ°å•†å“ç‰¹å¾æ–‡ä»¶: {item_feature_file}")
            return False

        # è½¬æ¢ä¸ºå­—å…¸ä¾¿äºæŸ¥æ‰¾
        self.user_feature_dict = self.user_features.set_index('user_id').to_dict('index')
        self.item_feature_dict = self.item_features.set_index('item_id').to_dict('index')

        return True

    def load_item_catalog(self):
        """åŠ è½½å•†å“ç›®å½•(På­é›†)"""
        print("ğŸ“š åŠ è½½å•†å“å­é›†P...")

        item_file = "dataset/tianchi_fresh_comp_train_item_online.txt"
        columns = ["item_id", "item_geohash", "item_category"]

        item_df = pd.read_csv(item_file, sep="\t", names=columns)
        # å¤„ç†ç©ºçš„geohash
        item_df['item_geohash'] = item_df['item_geohash'].fillna('unknown')

        self.p_items = set(item_df['item_id'].tolist())
        print(f"  ğŸ“¦ På­é›†å•†å“æ•°: {len(self.p_items):,}")
        print(f"  ğŸ·ï¸  å•†å“ç±»åˆ«æ•°: {item_df['item_category'].nunique()}")

        return item_df

    def extract_user_item_interactions(self):
        """æå–ç”¨æˆ·-å•†å“äº¤äº’å†å²"""
        print("\nğŸ“Š æå–ç”¨æˆ·-å•†å“äº¤äº’å†å²...")

        data_files = [
            ("data_1216.txt", 16),
            ("data_1217.txt", 17),
            ("data_1218.txt", 18)
        ]

        columns = ["user_id", "item_id", "behavior_type", "user_geohash", "item_category", "time"]

        for filename, day in data_files:
            print(f"\nğŸ“… å¤„ç†ç¬¬{day}å·æ•°æ®: {filename}")

            file_path = os.path.join(self.data_dir, filename)
            chunk_size = 1000000
            chunk_count = 0

            for chunk in pd.read_csv(file_path, sep="\t", names=columns, chunksize=chunk_size):
                chunk_count += 1
                print(f"  ğŸ”„ å¤„ç†å— {chunk_count} (è¡Œæ•°: {len(chunk):,})")

                # åªå¤„ç†På­é›†å•†å“å’Œæœ‰ç‰¹å¾çš„ç”¨æˆ·
                chunk = chunk[chunk['item_id'].isin(self.p_items)]
                chunk = chunk[chunk['user_id'].isin(self.user_feature_dict.keys())]

                if len(chunk) == 0:
                    continue

                # è§£ææ—¶é—´
                chunk['datetime'] = pd.to_datetime(chunk['time'], format="%Y-%m-%d %H", errors="coerce")
                chunk = chunk.dropna(subset=['datetime'])

                # æå–äº¤äº’ä¿¡æ¯
                for _, row in tqdm(chunk.iterrows(),
                                 total=len(chunk),
                                 desc=f"    æå–äº¤äº’",
                                 leave=False,
                                 miniters=len(chunk)//10):

                    user_id = int(row['user_id'])
                    item_id = int(row['item_id'])
                    behavior = int(row['behavior_type'])
                    timestamp = row['datetime']

                    # è®°å½•äº¤äº’
                    self.user_item_interactions[user_id][item_id].append({
                        'behavior': behavior,
                        'timestamp': timestamp,
                        'day': day
                    })

            print(f"  âœ… ç¬¬{day}å·æ•°æ®å¤„ç†å®Œæˆ")

        print(f"\nğŸ“Š äº¤äº’ç»Ÿè®¡:")
        total_users = len(self.user_item_interactions)
        total_pairs = sum(len(items) for items in self.user_item_interactions.values())
        print(f"  ğŸ‘¥ æœ‰äº¤äº’ç”¨æˆ·æ•°: {total_users:,}")
        print(f"  ğŸ”— ç”¨æˆ·-å•†å“å¯¹æ•°: {total_pairs:,}")

    def generate_interaction_features(self, user_id, item_id):
        """ç”Ÿæˆç”¨æˆ·-å•†å“äº¤äº’ç‰¹å¾"""
        interactions = self.user_item_interactions[user_id].get(item_id, [])

        # åŸºç¡€äº¤äº’ç‰¹å¾
        features = {
            'has_interaction': 1 if interactions else 0,
            'total_interactions': len(interactions),
            'browse_count': sum(1 for i in interactions if i['behavior'] == 1),
            'collect_count': sum(1 for i in interactions if i['behavior'] == 2),
            'cart_count': sum(1 for i in interactions if i['behavior'] == 3),
            'purchase_count': sum(1 for i in interactions if i['behavior'] == 4),
        }

        if interactions:
            # æ—¶é—´ç‰¹å¾
            timestamps = [i['timestamp'] for i in interactions]
            features['first_interaction_days_ago'] = (pd.Timestamp('2014-12-19') - min(timestamps)).days
            features['last_interaction_days_ago'] = (pd.Timestamp('2014-12-19') - max(timestamps)).days

            # æœ€è¿‘äº¤äº’
            recent_interactions = [i for i in interactions if i['day'] >= 17]  # æœ€è¿‘2å¤©
            features['recent_interactions'] = len(recent_interactions)
            features['recent_purchase_count'] = sum(1 for i in recent_interactions if i['behavior'] == 4)

            # è¡Œä¸ºè¿›å±• (æ˜¯å¦æœ‰è´­ä¹°å€¾å‘)
            features['max_behavior_type'] = max(i['behavior'] for i in interactions)
            features['behavior_progression'] = features['max_behavior_type'] / 4.0

            # äº¤äº’é¢‘ç‡
            if len(set(i['day'] for i in interactions)) > 1:
                features['interaction_frequency'] = len(interactions) / len(set(i['day'] for i in interactions))
            else:
                features['interaction_frequency'] = len(interactions)

        else:
            # æ— äº¤äº’çš„é»˜è®¤å€¼
            features.update({
                'first_interaction_days_ago': 999,
                'last_interaction_days_ago': 999,
                'recent_interactions': 0,
                'recent_purchase_count': 0,
                'max_behavior_type': 0,
                'behavior_progression': 0,
                'interaction_frequency': 0,
            })

        return features

    def generate_positive_samples(self):
        """ç”Ÿæˆæ­£æ ·æœ¬ï¼š18å·å®é™…è´­ä¹°çš„ç”¨æˆ·-å•†å“å¯¹"""
        print("\nâœ… ç”Ÿæˆæ­£æ ·æœ¬...")

        positive_samples = []

        # ä»18å·æ•°æ®ä¸­æ‰¾è´­ä¹°è¡Œä¸º
        file_path = os.path.join(self.data_dir, "data_1218.txt")
        columns = ["user_id", "item_id", "behavior_type", "user_geohash", "item_category", "time"]

        chunk_size = 1000000
        total_positive = 0

        for chunk in pd.read_csv(file_path, sep="\t", names=columns, chunksize=chunk_size):
            # åªè¦è´­ä¹°è¡Œä¸º
            purchase_chunk = chunk[chunk['behavior_type'] == 4]

            # åªå¤„ç†På­é›†å•†å“å’Œæœ‰ç‰¹å¾çš„ç”¨æˆ·
            purchase_chunk = purchase_chunk[purchase_chunk['item_id'].isin(self.p_items)]
            purchase_chunk = purchase_chunk[purchase_chunk['user_id'].isin(self.user_feature_dict.keys())]

            if len(purchase_chunk) == 0:
                continue

            for _, row in tqdm(purchase_chunk.iterrows(),
                             desc="ç”Ÿæˆæ­£æ ·æœ¬",
                             leave=False):
                user_id = int(row['user_id'])
                item_id = int(row['item_id'])

                # æ£€æŸ¥æ˜¯å¦æœ‰ç‰¹å¾
                if user_id in self.user_feature_dict and item_id in self.item_feature_dict:
                    positive_samples.append((user_id, item_id, 1))  # æ ‡ç­¾ä¸º1
                    total_positive += 1

        print(f"  âœ… æ­£æ ·æœ¬æ•°é‡: {total_positive:,}")
        return positive_samples

    def generate_negative_samples(self, positive_samples, ratio=3):
        """ç”Ÿæˆè´Ÿæ ·æœ¬ï¼šç”¨æˆ·äº¤äº’ä½†æœªè´­ä¹°çš„å•†å“ + éšæœºè´Ÿæ ·æœ¬"""
        print(f"\nâŒ ç”Ÿæˆè´Ÿæ ·æœ¬ (æ­£è´Ÿæ¯”ä¾‹ 1:{ratio})...")

        negative_samples = []
        positive_set = set((user_id, item_id) for user_id, item_id, _ in positive_samples)
        target_negative = len(positive_samples) * ratio

        # ç­–ç•¥1: ç”¨æˆ·äº¤äº’ä½†æœªè´­ä¹°çš„å•†å“
        print("  ğŸ“Š ç­–ç•¥1: äº¤äº’æœªè´­ä¹°")
        interaction_negatives = 0

        for user_id, item_interactions in tqdm(self.user_item_interactions.items(),
                                              desc="  æ‰«æäº¤äº’"):
            if user_id not in self.user_feature_dict:
                continue

            for item_id, interactions in item_interactions.items():
                if (user_id, item_id) in positive_set:
                    continue

                if item_id not in self.item_feature_dict:
                    continue

                # æ£€æŸ¥æ˜¯å¦æœ‰è´­ä¹°è¡Œä¸º
                has_purchase = any(i['behavior'] == 4 for i in interactions)
                if not has_purchase:
                    negative_samples.append((user_id, item_id, 0))
                    interaction_negatives += 1

                    if len(negative_samples) >= target_negative:
                        break

            if len(negative_samples) >= target_negative:
                break

        print(f"    äº¤äº’è´Ÿæ ·æœ¬: {interaction_negatives:,}")

        # ç­–ç•¥2: éšæœºè´Ÿæ ·æœ¬è¡¥å……
        if len(negative_samples) < target_negative:
            print("  ğŸ² ç­–ç•¥2: éšæœºè´Ÿæ ·æœ¬")

            users_with_features = list(self.user_feature_dict.keys())
            items_with_features = list(self.item_feature_dict.keys())

            remaining = target_negative - len(negative_samples)
            attempts = 0
            max_attempts = remaining * 10  # é˜²æ­¢æ— é™å¾ªç¯

            for _ in tqdm(range(remaining), desc="  ç”Ÿæˆéšæœºè´Ÿæ ·æœ¬"):
                if attempts > max_attempts:
                    break

                user_id = random.choice(users_with_features)
                item_id = random.choice(items_with_features)
                attempts += 1

                if (user_id, item_id) not in positive_set and (user_id, item_id, 0) not in negative_samples:
                    negative_samples.append((user_id, item_id, 0))

        print(f"  âŒ è´Ÿæ ·æœ¬æ€»æ•°: {len(negative_samples):,}")
        return negative_samples[:target_negative]

    def build_training_samples(self):
        """æ„å»ºå®Œæ•´çš„è®­ç»ƒæ ·æœ¬"""
        print("\nğŸ”§ æ„å»ºè®­ç»ƒæ ·æœ¬...")

        # ç”Ÿæˆæ­£è´Ÿæ ·æœ¬
        positive_samples = self.generate_positive_samples()
        negative_samples = self.generate_negative_samples(positive_samples)

        # åˆå¹¶æ ·æœ¬
        all_samples = positive_samples + negative_samples
        print(f"\nğŸ“Š æ ·æœ¬ç»Ÿè®¡:")
        print(f"  âœ… æ­£æ ·æœ¬: {len(positive_samples):,}")
        print(f"  âŒ è´Ÿæ ·æœ¬: {len(negative_samples):,}")
        print(f"  ğŸ“ æ€»æ ·æœ¬: {len(all_samples):,}")

        # éšæœºæ‰“ä¹±
        random.shuffle(all_samples)

        # æ„å»ºç‰¹å¾çŸ©é˜µ
        print(f"\nğŸ—ï¸  æ„å»ºç‰¹å¾çŸ©é˜µ...")
        feature_data = []

        for user_id, item_id, label in tqdm(all_samples, desc="æ„å»ºç‰¹å¾"):
            # åŸºç¡€ID
            sample = {
                'user_id': user_id,
                'item_id': item_id,
                'label': label
            }

            # ç”¨æˆ·ç‰¹å¾ (39ç»´)
            user_features = self.user_feature_dict.get(user_id, {})
            for key, value in user_features.items():
                sample[f'user_{key}'] = value

            # å•†å“ç‰¹å¾
            item_features = self.item_feature_dict.get(item_id, {})
            for key, value in item_features.items():
                if key != 'item_id':  # é¿å…é‡å¤
                    sample[f'item_{key}'] = value

            # äº¤äº’ç‰¹å¾
            interaction_features = self.generate_interaction_features(user_id, item_id)
            for key, value in interaction_features.items():
                sample[f'interaction_{key}'] = value

            feature_data.append(sample)

        # è½¬æ¢ä¸ºDataFrame
        feature_df = pd.DataFrame(feature_data)

        print(f"âœ… ç‰¹å¾çŸ©é˜µæ„å»ºå®Œæˆ:")
        print(f"  ğŸ“ æ ·æœ¬æ•°: {len(feature_df):,}")
        print(f"  ğŸ”§ ç‰¹å¾æ•°: {len(feature_df.columns)-3}")  # å‡å»user_id, item_id, label
        print(f"  ğŸ¯ æ­£æ ·æœ¬æ¯”ä¾‹: {feature_df['label'].mean():.3f}")

        return feature_df

    def export_training_data(self, feature_df):
        """å¯¼å‡ºè®­ç»ƒæ•°æ®"""
        print("\nğŸ’¾ å¯¼å‡ºè®­ç»ƒæ•°æ®...")

        # ä¿å­˜å®Œæ•´æ•°æ®
        train_file = os.path.join(self.output_dir, "training_samples.csv")
        feature_df.to_csv(train_file, index=False)

        print(f"âœ… è®­ç»ƒæ•°æ®å·²ä¿å­˜: {train_file}")

        # ç‰¹å¾ç»Ÿè®¡
        feature_cols = [col for col in feature_df.columns if col not in ['user_id', 'item_id', 'label']]
        print(f"\nğŸ“‹ ç‰¹å¾ç»„åˆ†æ:")
        print(f"  ğŸ‘¥ ç”¨æˆ·ç‰¹å¾æ•°: {len([col for col in feature_cols if col.startswith('user_')])}")
        print(f"  ğŸ“¦ å•†å“ç‰¹å¾æ•°: {len([col for col in feature_cols if col.startswith('item_')])}")
        print(f"  ğŸ”— äº¤äº’ç‰¹å¾æ•°: {len([col for col in feature_cols if col.startswith('interaction_')])}")

        # æ•°æ®è´¨é‡æ£€æŸ¥
        print(f"\nğŸ” æ•°æ®è´¨é‡æ£€æŸ¥:")
        print(f"  ğŸ“Š ç¼ºå¤±å€¼: {feature_df.isnull().sum().sum()}")
        print(f"  ğŸ“ˆ æ•°å€¼åˆ—æ•°: {len(feature_df.select_dtypes(include=[np.number]).columns)}")

        # ä¿å­˜ç‰¹å¾ååˆ—è¡¨
        feature_names_file = os.path.join(self.output_dir, "feature_names.txt")
        with open(feature_names_file, 'w') as f:
            for col in feature_cols:
                f.write(f"{col}\n")
        print(f"  ğŸ“ ç‰¹å¾åå·²ä¿å­˜: {feature_names_file}")

        return train_file


def main():
    """ä¸»å‡½æ•°"""
    print("=== è®­ç»ƒæ ·æœ¬ç”Ÿæˆå™¨ ===")
    print("ğŸ¯ ç›®æ ‡ï¼šç”Ÿæˆç”¨æˆ·-å•†å“äº¤äº’é¢„æµ‹è®­ç»ƒæ ·æœ¬")

    generator = TrainingSampleGenerator()

    # 1. åŠ è½½ç‰¹å¾
    if not generator.load_features():
        print("âŒ ç‰¹å¾åŠ è½½å¤±è´¥")
        return

    # 2. åŠ è½½å•†å“ç›®å½•
    generator.load_item_catalog()

    # 3. æå–äº¤äº’å†å²
    generator.extract_user_item_interactions()

    # 4. æ„å»ºè®­ç»ƒæ ·æœ¬
    feature_df = generator.build_training_samples()

    # 5. å¯¼å‡ºæ•°æ®
    train_file = generator.export_training_data(feature_df)

    print(f"\nğŸ‰ è®­ç»ƒæ ·æœ¬ç”Ÿæˆå®Œæˆ!")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {train_file}")
    print(f"ä¸‹ä¸€æ­¥ï¼šè®­ç»ƒLightGBMæ¨¡å‹")


if __name__ == "__main__":
    main()