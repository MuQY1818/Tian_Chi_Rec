#!/usr/bin/env python3
"""
ç®€åŒ–å•†å“ç‰¹å¾æå–å™¨
å¿«é€Ÿæå–åŸºç¡€å•†å“ç‰¹å¾ï¼Œé…åˆç°æœ‰39ç»´ç”¨æˆ·ç‰¹å¾
"""

import sys
import pandas as pd
import numpy as np
from collections import defaultdict
import os
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

sys.path.append('src')


class SimpleItemFeatureExtractor:
    """ç®€åŒ–å•†å“ç‰¹å¾æå–å™¨"""

    def __init__(self, data_dir="dataset/preprocess_16to18", output_dir="/mnt/data/tianchi_features"):
        self.data_dir = data_dir
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)

        # åªä¿ç•™æ ¸å¿ƒç»Ÿè®¡
        self.item_stats = defaultdict(lambda: {
            'total_interactions': 0,
            'purchase_count': 0,
            'unique_users': set(),
            'category': 0
        })

        print(f"ğŸ“ æ•°æ®ç›®å½•: {data_dir}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")

    def load_item_catalog(self):
        """åŠ è½½å•†å“ç›®å½•ï¼ˆPå­é›†ï¼‰"""
        print("ğŸ“š åŠ è½½å•†å“å­é›†P...")

        item_file = "dataset/tianchi_fresh_comp_train_item_online.txt"
        columns = ["item_id", "item_geohash", "item_category"]

        item_df = pd.read_csv(item_file, sep="\t", names=columns)
        # å¤„ç†ç©ºçš„geohash
        item_df['item_geohash'] = item_df['item_geohash'].fillna('unknown')

        self.item_catalog = dict(zip(item_df['item_id'], item_df['item_category']))

        print(f"  ğŸ“¦ På­é›†å•†å“æ•°: {len(self.item_catalog):,}")
        print(f"  ğŸ·ï¸  å•†å“ç±»åˆ«æ•°: {item_df['item_category'].nunique()}")
        print(f"  ğŸ“ æœ‰åœ°ç†ä¿¡æ¯çš„å•†å“: {(item_df['item_geohash'] != 'unknown').sum():,}")
        return item_df

    def process_data_files(self):
        """å¿«é€Ÿå¤„ç†æ•°æ®æ–‡ä»¶ï¼Œåªç»Ÿè®¡æ ¸å¿ƒæŒ‡æ ‡"""
        print("\nğŸ“‚ å¿«é€Ÿå¤„ç†16-18å·æ•°æ®...")

        columns = ["user_id", "item_id", "behavior_type", "user_geohash", "item_category", "time"]

        for day in [16, 17, 18]:
            filename = f"data_12{day}.txt"
            print(f"\nğŸ“… å¤„ç†ç¬¬{day}å·æ•°æ®: {filename}")

            file_path = os.path.join(self.data_dir, filename)

            # å¤§å—è¯»å–æé«˜æ•ˆç‡
            chunk_size = 2000000
            processed_lines = 0

            for chunk in pd.read_csv(file_path, sep="\t", names=columns, chunksize=chunk_size):
                # åªå¤„ç†På­é›†å•†å“
                chunk = chunk[chunk['item_id'].isin(self.item_catalog.keys())]

                if len(chunk) == 0:
                    continue

                # å¿«é€Ÿç»Ÿè®¡
                for _, row in chunk.iterrows():
                    item_id = int(row['item_id'])
                    user_id = int(row['user_id'])
                    behavior = int(row['behavior_type'])

                    stats = self.item_stats[item_id]
                    stats['total_interactions'] += 1
                    stats['unique_users'].add(user_id)
                    stats['category'] = self.item_catalog[item_id]

                    if behavior == 4:  # è´­ä¹°
                        stats['purchase_count'] += 1

                processed_lines += len(chunk)

                if processed_lines % 1000000 == 0:
                    print(f"    ğŸ“ˆ å·²å¤„ç†: {processed_lines:,} è¡Œ | ç”¨æˆ·-å•†å“å¯¹: {len(self.item_stats):,}")

            print(f"  âœ… ç¬¬{day}å·æ•°æ®å®Œæˆï¼Œå¤„ç†äº† {processed_lines:,} è¡Œ")

        print(f"\nğŸ“Š ç»Ÿè®¡å®Œæˆ:")
        print(f"  ğŸ“¦ æœ‰æ•°æ®å•†å“æ•°: {len(self.item_stats):,}")

    def generate_simple_features(self):
        """ç”Ÿæˆç®€åŒ–çš„å•†å“ç‰¹å¾ï¼ˆåªä¿ç•™æ ¸å¿ƒ8ç»´ï¼‰"""
        print("\nğŸ”§ ç”Ÿæˆç®€åŒ–å•†å“ç‰¹å¾...")

        features_list = []

        for item_id, stats in tqdm(self.item_stats.items(), desc="ç”Ÿæˆç‰¹å¾"):
            features = {
                'item_id': item_id,
                'item_category': stats['category'],

                # æ ¸å¿ƒç‰¹å¾ï¼ˆ8ç»´ï¼‰
                'popularity': stats['total_interactions'],  # æµè¡Œåº¦
                'user_count': len(stats['unique_users']),   # ç”¨æˆ·æ•°
                'purchase_count': stats['purchase_count'],   # è´­ä¹°æ•°
                'purchase_rate': stats['purchase_count'] / max(stats['total_interactions'], 1),  # è´­ä¹°ç‡
                'avg_user_interactions': stats['total_interactions'] / max(len(stats['unique_users']), 1),  # å¹³å‡ç”¨æˆ·äº¤äº’

                # ç®€å•åˆ†ç®±ç‰¹å¾
                'popularity_level': self._get_popularity_level(stats['total_interactions']),
                'purchase_level': self._get_purchase_level(stats['purchase_count']),
                'category_id': stats['category']  # ç±»åˆ«IDä½œä¸ºåˆ†ç±»ç‰¹å¾
            }

            features_list.append(features)

        feature_df = pd.DataFrame(features_list)

        # æ·»åŠ ç±»åˆ«çº§åˆ«ç»Ÿè®¡
        category_stats = feature_df.groupby('item_category').agg({
            'popularity': 'mean',
            'purchase_rate': 'mean'
        }).reset_index()
        category_stats.columns = ['item_category', 'category_avg_popularity', 'category_avg_purchase_rate']

        # åˆå¹¶ç±»åˆ«ç‰¹å¾
        feature_df = feature_df.merge(category_stats, on='item_category', how='left')

        print(f"âœ… å•†å“ç‰¹å¾ç”Ÿæˆå®Œæˆ:")
        print(f"  ğŸ“¦ å•†å“æ•°: {len(feature_df):,}")
        print(f"  ğŸ”§ ç‰¹å¾æ•°: {len(feature_df.columns)-1}")

        return feature_df

    def _get_popularity_level(self, interactions):
        """æµè¡Œåº¦åˆ†çº§"""
        if interactions >= 10000:
            return 4  # è¶…çƒ­é—¨
        elif interactions >= 1000:
            return 3  # çƒ­é—¨
        elif interactions >= 100:
            return 2  # ä¸€èˆ¬
        elif interactions >= 10:
            return 1  # å†·é—¨
        else:
            return 0  # è¶…å†·é—¨

    def _get_purchase_level(self, purchases):
        """è´­ä¹°é‡åˆ†çº§"""
        if purchases >= 100:
            return 3  # é«˜è´­ä¹°
        elif purchases >= 10:
            return 2  # ä¸­è´­ä¹°
        elif purchases >= 1:
            return 1  # ä½è´­ä¹°
        else:
            return 0  # æ— è´­ä¹°

    def export_features(self):
        """å¯¼å‡ºç®€åŒ–ç‰¹å¾"""
        print("\nğŸ’¾ å¯¼å‡ºå•†å“ç‰¹å¾...")

        feature_df = self.generate_simple_features()

        # ä¿å­˜ç‰¹å¾
        output_file = os.path.join(self.output_dir, "simple_item_features.csv")
        feature_df.to_csv(output_file, index=False)

        print(f"âœ… å•†å“ç‰¹å¾å·²ä¿å­˜: {output_file}")

        # ç‰¹å¾ç»Ÿè®¡
        print(f"\nğŸ“Š ç‰¹å¾ç»Ÿè®¡:")
        print(f"  ğŸ”¥ æœ€é«˜æµè¡Œåº¦: {feature_df['popularity'].max():,}")
        print(f"  ğŸ’° æœ€é«˜è´­ä¹°ç‡: {feature_df['purchase_rate'].max():.3f}")
        print(f"  ğŸ“Š å¹³å‡æµè¡Œåº¦: {feature_df['popularity'].mean():.1f}")
        print(f"  ğŸ·ï¸  ç±»åˆ«æ•°: {feature_df['item_category'].nunique()}")

        # æ˜¾ç¤ºç¤ºä¾‹
        print(f"\nğŸ“‹ ç‰¹å¾ç¤ºä¾‹:")
        sample_cols = ['item_id', 'popularity', 'purchase_count', 'purchase_rate', 'popularity_level']
        print(feature_df[sample_cols].head())

        return feature_df


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ“¦ === ç®€åŒ–å•†å“ç‰¹å¾æå–å™¨ === ğŸ“¦")
    print("ğŸ¯ ç›®æ ‡ï¼šå¿«é€Ÿæå–æ ¸å¿ƒå•†å“ç‰¹å¾ï¼Œé…åˆç°æœ‰ç”¨æˆ·ç‰¹å¾")
    print("âš¡ é¢„è®¡è€—æ—¶ï¼š2-3åˆ†é’Ÿ")
    print("â”" * 50)

    extractor = SimpleItemFeatureExtractor()

    # 1. åŠ è½½å•†å“ç›®å½•
    extractor.load_item_catalog()

    # 2. å¿«é€Ÿå¤„ç†æ•°æ®
    extractor.process_data_files()

    # 3. ç”Ÿæˆå¹¶å¯¼å‡ºç‰¹å¾
    feature_df = extractor.export_features()

    print(f"\nğŸ‰ ç®€åŒ–å•†å“ç‰¹å¾æå–å®Œæˆ!")
    print(f"âš¡ æ¯”å®Œæ•´ç‰ˆæœ¬å¿«3-5å€")
    print(f"ğŸ“ è¾“å‡º: {extractor.output_dir}/simple_item_features.csv")


if __name__ == "__main__":
    main()