#!/usr/bin/env python3
"""
åˆ†æ‰¹ç”¨æˆ·ç‰¹å¾æå–å™¨
å¤„ç†å…¨é‡æ•°æ®ï¼Œä¸ºæ¯ä¸ªç”¨æˆ·ç”Ÿæˆç»Ÿè®¡ç‰¹å¾
"""

import sys
import pandas as pd
import numpy as np
from collections import defaultdict, Counter
import pickle
import os
from tqdm import tqdm
import gc

sys.path.append('src')


class BatchUserFeatureExtractor:
    """åˆ†æ‰¹ç”¨æˆ·ç‰¹å¾æå–å™¨"""

    def __init__(self, output_dir="/mnt/data/tianchi_features"):
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")

        self.user_stats = defaultdict(lambda: {
            # åŸºç¡€ç»Ÿè®¡
            'total_actions': 0,
            'browse_count': 0,
            'collect_count': 0,
            'cart_count': 0,
            'purchase_count': 0,

            # å•†å“ç›¸å…³
            'unique_items': set(),
            'unique_categories': set(),
            'item_interactions': defaultdict(int),
            'category_preferences': defaultdict(int),

            # æ—¶é—´æ¨¡å¼
            'hour_activity': defaultdict(int),
            'day_activity': defaultdict(int),
            'first_action_time': None,
            'last_action_time': None,

            # åœ°ç†ä½ç½®
            'geo_locations': defaultdict(int),
            'primary_geo': None,

            # è¡Œä¸ºåºåˆ—
            'behavior_sequence': [],
            'recent_behaviors': [],  # æœ€è¿‘3å¤©
        })

    def process_data_batch(self, file_path, batch_size=1000000):
        """åˆ†æ‰¹å¤„ç†å•ä¸ªæ•°æ®æ–‡ä»¶"""
        print(f"ğŸ“‚ å¤„ç†æ–‡ä»¶: {file_path}")

        # å…ˆè®¡ç®—æ–‡ä»¶æ€»è¡Œæ•°
        print("  ğŸ“ è®¡ç®—æ–‡ä»¶å¤§å°...")
        total_lines = sum(1 for _ in open(file_path, 'r'))
        estimated_chunks = (total_lines // batch_size) + 1
        print(f"  ğŸ“Š æ–‡ä»¶æ€»è¡Œæ•°: {total_lines:,}")
        print(f"  ğŸ”¢ é¢„è®¡æ‰¹æ¬¡æ•°: {estimated_chunks}")

        columns = ["user_id", "item_id", "behavior_type", "user_geohash", "item_category", "time"]
        chunk_reader = pd.read_csv(file_path, sep="\t", names=columns, chunksize=batch_size)

        processed_rows = 0
        valid_rows = 0

        for chunk_num, chunk in enumerate(chunk_reader):
            processed_rows += len(chunk)
            progress_pct = (processed_rows / total_lines) * 100

            print(f"\n  ğŸ”„ æ‰¹æ¬¡ {chunk_num + 1}/{estimated_chunks}")
            print(f"     ğŸ“¥ åŸå§‹è¡Œæ•°: {len(chunk):,}")
            print(f"     ğŸ“ˆ æ€»è¿›åº¦: {progress_pct:.1f}% ({processed_rows:,}/{total_lines:,})")

            # æ•°æ®æ¸…æ´—
            print(f"     ğŸ§¹ æ•°æ®æ¸…æ´—ä¸­...")
            chunk = chunk[chunk["behavior_type"].isin([1, 2, 3, 4])]
            chunk = chunk.dropna(subset=["user_id", "item_id", "time"])
            print(f"     âœ“ æ¸…æ´—å: {len(chunk):,} è¡Œ")

            # è§£ææ—¶é—´
            print(f"     ğŸ• è§£ææ—¶é—´ä¸­...")
            chunk["datetime"] = pd.to_datetime(chunk["time"], format="%Y-%m-%d %H", errors="coerce")
            chunk = chunk.dropna(subset=["datetime"])
            print(f"     âœ“ æ—¶é—´è§£æå: {len(chunk):,} è¡Œ")

            # æ·»åŠ æ—¶é—´ç‰¹å¾
            chunk["hour"] = chunk["datetime"].dt.hour
            chunk["date"] = chunk["datetime"].dt.date
            chunk["weekday"] = chunk["datetime"].dt.weekday

            # å¤„ç†æ¯ä¸€è¡Œæ•°æ®
            print(f"     ğŸ‘¥ æ›´æ–°ç”¨æˆ·ç»Ÿè®¡ä¸­...")
            users_in_chunk = set()

            for idx, row in tqdm(chunk.iterrows(),
                               total=len(chunk),
                               desc=f"     å¤„ç†è¡Œæ•°",
                               leave=False,
                               miniters=len(chunk)//10):
                self._update_user_stats(row)
                users_in_chunk.add(int(row["user_id"]))

            valid_rows += len(chunk)

            print(f"     âœ… æ‰¹æ¬¡å®Œæˆ")
            print(f"     ğŸ‘¤ æ–°å¢ç”¨æˆ·æ•°: {len(users_in_chunk)}")
            print(f"     ğŸ“Š ç´¯è®¡ç”¨æˆ·æ•°: {len(self.user_stats)}")
            print(f"     ğŸ“ ç´¯è®¡æœ‰æ•ˆè¡Œæ•°: {valid_rows:,}")

            # æ¯5ä¸ªæ‰¹æ¬¡ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
            if (chunk_num + 1) % 5 == 0:
                checkpoint_name = os.path.join(self.output_dir, f"checkpoint_batch_{chunk_num + 1}.pkl")
                print(f"     ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_name}")
                self.save_checkpoint(checkpoint_name)
                print(f"     âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜")

            # å†…å­˜æ¸…ç†
            del chunk
            gc.collect()

        print(f"\n  ğŸ‰ æ–‡ä»¶å¤„ç†å®Œæˆ!")
        print(f"     ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        print(f"     - å¤„ç†è¡Œæ•°: {processed_rows:,}")
        print(f"     - æœ‰æ•ˆè¡Œæ•°: {valid_rows:,}")
        print(f"     - ç”¨æˆ·æ•°é‡: {len(self.user_stats):,}")
        print(f"     - æ•°æ®åˆ©ç”¨ç‡: {(valid_rows/processed_rows)*100:.1f}%")

    def _update_user_stats(self, row):
        """æ›´æ–°å•ä¸ªç”¨æˆ·çš„ç»Ÿè®¡ä¿¡æ¯"""
        user_id = int(row["user_id"])
        item_id = int(row["item_id"])
        behavior = int(row["behavior_type"])
        category = row["item_category"]
        geo = row["user_geohash"] if pd.notna(row["user_geohash"]) else "unknown"
        datetime_obj = row["datetime"]
        hour = int(row["hour"])
        date = row["date"]

        stats = self.user_stats[user_id]

        # åŸºç¡€ç»Ÿè®¡
        stats['total_actions'] += 1
        if behavior == 1:
            stats['browse_count'] += 1
        elif behavior == 2:
            stats['collect_count'] += 1
        elif behavior == 3:
            stats['cart_count'] += 1
        elif behavior == 4:
            stats['purchase_count'] += 1

        # å•†å“å’Œç±»åˆ«
        stats['unique_items'].add(item_id)
        if pd.notna(category):
            stats['unique_categories'].add(category)
            stats['category_preferences'][category] += 1
        stats['item_interactions'][item_id] += 1

        # æ—¶é—´æ¨¡å¼
        stats['hour_activity'][hour] += 1
        stats['day_activity'][date] += 1

        if stats['first_action_time'] is None or datetime_obj < stats['first_action_time']:
            stats['first_action_time'] = datetime_obj
        if stats['last_action_time'] is None or datetime_obj > stats['last_action_time']:
            stats['last_action_time'] = datetime_obj

        # åœ°ç†ä½ç½®
        stats['geo_locations'][geo] += 1
        if stats['primary_geo'] is None or stats['geo_locations'][geo] > stats['geo_locations'].get(stats['primary_geo'], 0):
            stats['primary_geo'] = geo

        # è¡Œä¸ºåºåˆ— (ä¿ç•™æœ€è¿‘1000ä¸ª)
        stats['behavior_sequence'].append((datetime_obj, behavior, item_id))
        if len(stats['behavior_sequence']) > 1000:
            stats['behavior_sequence'] = stats['behavior_sequence'][-1000:]

        # æœ€è¿‘è¡Œä¸º (æœ€è¿‘3å¤©)
        recent_threshold = pd.to_datetime("2014-12-18") - pd.Timedelta(days=3)
        if datetime_obj >= recent_threshold:
            stats['recent_behaviors'].append((datetime_obj, behavior, item_id))

    def generate_user_features(self):
        """ä»åŸå§‹ç»Ÿè®¡ç”Ÿæˆæœºå™¨å­¦ä¹ ç‰¹å¾"""
        print("ğŸ”§ ç”Ÿæˆç”¨æˆ·ç‰¹å¾...")

        features_list = []

        for user_id, stats in tqdm(self.user_stats.items(), desc="ç”Ÿæˆç‰¹å¾"):
            features = {
                'user_id': user_id,

                # åŸºç¡€æ´»è·ƒåº¦ç‰¹å¾
                'total_actions': stats['total_actions'],
                'browse_count': stats['browse_count'],
                'collect_count': stats['collect_count'],
                'cart_count': stats['cart_count'],
                'purchase_count': stats['purchase_count'],

                # è½¬åŒ–ç‡ç‰¹å¾
                'collect_rate': stats['collect_count'] / max(stats['browse_count'], 1),
                'cart_rate': stats['cart_count'] / max(stats['browse_count'], 1),
                'purchase_rate': stats['purchase_count'] / max(stats['browse_count'], 1),
                'purchase_conversion': stats['purchase_count'] / max(stats['total_actions'], 1),

                # å•†å“å’Œç±»åˆ«å¤šæ ·æ€§
                'unique_items_count': len(stats['unique_items']),
                'unique_categories_count': len(stats['unique_categories']),
                'avg_interactions_per_item': stats['total_actions'] / max(len(stats['unique_items']), 1),

                # æ—¶é—´æ¨¡å¼ç‰¹å¾
                'active_days': len(stats['day_activity']),
                'avg_daily_actions': stats['total_actions'] / max(len(stats['day_activity']), 1),
                'active_hours_count': len(stats['hour_activity']),

                # æ—¶é—´åå¥½
                'morning_activity': sum(stats['hour_activity'][h] for h in range(6, 12)) / max(stats['total_actions'], 1),
                'afternoon_activity': sum(stats['hour_activity'][h] for h in range(12, 18)) / max(stats['total_actions'], 1),
                'evening_activity': sum(stats['hour_activity'][h] for h in range(18, 24)) / max(stats['total_actions'], 1),
                'night_activity': sum(stats['hour_activity'][h] for h in range(0, 6)) / max(stats['total_actions'], 1),

                # åœ°ç†ç‰¹å¾
                'unique_geo_count': len(stats['geo_locations']),
                'geo_concentration': max(stats['geo_locations'].values()) / max(stats['total_actions'], 1) if stats['geo_locations'] else 0,

                # æœ€è¿‘æ´»è·ƒåº¦
                'recent_actions_count': len(stats['recent_behaviors']),
                'recent_purchase_count': sum(1 for _, behavior, _ in stats['recent_behaviors'] if behavior == 4),
                'days_since_last_action': 0,  # ä¼šåœ¨åé¢è®¡ç®—
                'days_since_first_action': 0,  # ä¼šåœ¨åé¢è®¡ç®—
            }

            # è®¡ç®—æ—¶é—´é—´éš”
            if stats['last_action_time']:
                features['days_since_last_action'] = (pd.to_datetime("2014-12-18") - stats['last_action_time']).days
            if stats['first_action_time']:
                features['days_since_first_action'] = (pd.to_datetime("2014-12-18") - stats['first_action_time']).days

            # æœ€å–œæ¬¢çš„ç±»åˆ«
            if stats['category_preferences']:
                top_category = max(stats['category_preferences'], key=stats['category_preferences'].get)
                features['top_category'] = top_category
                features['top_category_ratio'] = stats['category_preferences'][top_category] / max(stats['total_actions'], 1)
            else:
                features['top_category'] = -1
                features['top_category_ratio'] = 0

            # è¡Œä¸ºè§„å¾‹æ€§
            if len(stats['hour_activity']) > 1:
                hour_counts = np.array(list(stats['hour_activity'].values()))
                features['activity_regularity'] = 1 - (hour_counts.std() / max(hour_counts.mean(), 1))
            else:
                features['activity_regularity'] = 0

            # è´­ä¹°é¢„æµ‹ç‰¹å¾ (åŸºäºæœ€è¿‘è¡Œä¸º)
            features['likely_to_purchase'] = self._predict_purchase_likelihood(stats)

            features_list.append(features)

        return pd.DataFrame(features_list)

    def _predict_purchase_likelihood(self, stats):
        """é¢„æµ‹è´­ä¹°å¯èƒ½æ€§ (å¯å‘å¼)"""
        score = 0

        # å†å²è´­ä¹°è¡Œä¸ºæƒé‡
        if stats['purchase_count'] > 0:
            score += 0.4
        if stats['purchase_count'] > 2:
            score += 0.2

        # æœ€è¿‘æ´»è·ƒåº¦
        if len(stats['recent_behaviors']) > 5:
            score += 0.2

        # æœ€è¿‘è´­ä¹°è¡Œä¸º
        recent_purchases = sum(1 for _, behavior, _ in stats['recent_behaviors'] if behavior == 4)
        if recent_purchases > 0:
            score += 0.3

        # é«˜è½¬åŒ–è¡Œä¸º
        if stats['cart_count'] > 0:
            score += 0.1
        if stats['collect_count'] > 0:
            score += 0.05

        return min(score, 1.0)

    def save_checkpoint(self, filename=None):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        if filename is None:
            filename = os.path.join(self.output_dir, "user_stats_checkpoint.pkl")

        print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {filename}")
        with open(filename, 'wb') as f:
            pickle.dump(dict(self.user_stats), f)

    def load_checkpoint(self, filename=None):
        """åŠ è½½æœ€æ–°çš„æ£€æŸ¥ç‚¹"""
        # åœ¨è¾“å‡ºç›®å½•ä¸­å¯»æ‰¾æ‰€æœ‰å¯èƒ½çš„æ£€æŸ¥ç‚¹æ–‡ä»¶
        checkpoint_files = []

        # é»˜è®¤æ£€æŸ¥ç‚¹
        if filename is None:
            filename = os.path.join(self.output_dir, "user_stats_checkpoint.pkl")
        if os.path.exists(filename):
            checkpoint_files.append(filename)

        # æ‰¹æ¬¡æ£€æŸ¥ç‚¹
        import glob
        batch_checkpoints = glob.glob(os.path.join(self.output_dir, "checkpoint_batch_*.pkl"))
        checkpoint_files.extend(batch_checkpoints)

        # æ–‡ä»¶æ£€æŸ¥ç‚¹
        file_checkpoints = glob.glob(os.path.join(self.output_dir, "checkpoint_after_file_*.pkl"))
        checkpoint_files.extend(file_checkpoints)

        if checkpoint_files:
            # é€‰æ‹©æœ€æ–°çš„æ£€æŸ¥ç‚¹
            latest_checkpoint = max(checkpoint_files, key=os.path.getmtime)
            print(f"ğŸ“‚ æ‰¾åˆ° {len(checkpoint_files)} ä¸ªæ£€æŸ¥ç‚¹æ–‡ä»¶")
            print(f"ğŸ“‚ åŠ è½½æœ€æ–°æ£€æŸ¥ç‚¹: {latest_checkpoint}")

            with open(latest_checkpoint, 'rb') as f:
                loaded_stats = pickle.load(f)
                for user_id, stats in loaded_stats.items():
                    self.user_stats[user_id] = stats

            print(f"âœ… æ£€æŸ¥ç‚¹åŠ è½½å®Œæˆ")
            print(f"  ğŸ‘¥ æ¢å¤ç”¨æˆ·æ•°: {len(self.user_stats):,}")
            total_actions = sum(stats['total_actions'] for stats in self.user_stats.values())
            print(f"  ğŸ“ˆ æ¢å¤äº¤äº’æ•°: {total_actions:,}")

            return True

        print("ğŸ“‚ æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œä»å¤´å¼€å§‹å¤„ç†")
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("=== åˆ†æ‰¹ç”¨æˆ·ç‰¹å¾æå–å™¨ ===")
    print("ğŸ¯ ç›®æ ‡ï¼šå¤„ç†å…¨é‡11.65äº¿è¡Œæ•°æ®ï¼Œç”Ÿæˆç”¨æˆ·ç‰¹å¾")

    extractor = BatchUserFeatureExtractor()

    # å°è¯•åŠ è½½æ£€æŸ¥ç‚¹
    if not extractor.load_checkpoint():
        print("\nğŸš€ å¼€å§‹å…¨é‡æ•°æ®å¤„ç†...")

        # å¤„ç†æ•°æ®æ–‡ä»¶
        files = [
            "dataset/tianchi_fresh_comp_train_user_online_partA.txt",
            "dataset/tianchi_fresh_comp_train_user_online_partB.txt"
        ]

        print(f"ğŸ“ å¾…å¤„ç†æ–‡ä»¶æ•°: {len(files)}")

        for file_idx, file_path in enumerate(files, 1):
            print(f"\n{'='*60}")
            print(f"ğŸ—‚ï¸  æ–‡ä»¶ {file_idx}/{len(files)}: {file_path}")
            print(f"{'='*60}")

            if os.path.exists(file_path):
                # æ˜¾ç¤ºæ–‡ä»¶å¤§å°
                file_size = os.path.getsize(file_path) / (1024**3)  # GB
                print(f"ğŸ“ æ–‡ä»¶å¤§å°: {file_size:.1f} GB")

                extractor.process_data_batch(file_path)

                # æ¯å¤„ç†å®Œä¸€ä¸ªæ–‡ä»¶ä¿å­˜æ£€æŸ¥ç‚¹
                print(f"\nğŸ’¾ ä¿å­˜è¿›åº¦æ£€æŸ¥ç‚¹...")
                checkpoint_path = os.path.join(extractor.output_dir, f"checkpoint_after_file_{file_idx}.pkl")
                extractor.save_checkpoint(checkpoint_path)

                print(f"\nğŸ“Š å½“å‰è¿›åº¦æ±‡æ€»:")
                print(f"  âœ… å·²å®Œæˆæ–‡ä»¶: {file_idx}/{len(files)}")
                print(f"  ğŸ‘¥ ç´¯è®¡ç”¨æˆ·æ•°: {len(extractor.user_stats):,}")
                total_actions = sum(stats['total_actions'] for stats in extractor.user_stats.values())
                print(f"  ğŸ“ˆ ç´¯è®¡äº¤äº’æ•°: {total_actions:,}")
                print(f"  ğŸ’¾ å†…å­˜å ç”¨: {len(extractor.user_stats) * 0.001:.1f} MB (ä¼°ç®—)")

            else:
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

        print(f"\nğŸ‰ å…¨éƒ¨æ–‡ä»¶å¤„ç†å®Œæˆ!")

    else:
        print("ğŸ“‚ ä»æ£€æŸ¥ç‚¹æ¢å¤æ•°æ®...")

    print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯:")
    print(f"  ğŸ‘¥ æ€»ç”¨æˆ·æ•°: {len(extractor.user_stats):,}")
    total_actions = sum(stats['total_actions'] for stats in extractor.user_stats.values())
    print(f"  ğŸ“ˆ æ€»äº¤äº’æ•°: {total_actions:,}")
    avg_actions = total_actions / len(extractor.user_stats) if extractor.user_stats else 0
    print(f"  ğŸ“Š å¹³å‡æ¯ç”¨æˆ·äº¤äº’æ•°: {avg_actions:.1f}")

    # ç”Ÿæˆç‰¹å¾
    print(f"\nğŸ”§ å¼€å§‹ç”Ÿæˆæœºå™¨å­¦ä¹ ç‰¹å¾...")
    features_df = extractor.generate_user_features()

    # ä¿å­˜ç‰¹å¾
    print(f"\nğŸ’¾ ä¿å­˜ç‰¹å¾æ–‡ä»¶...")
    feature_file_path = os.path.join(extractor.output_dir, "user_features_full.csv")
    features_df.to_csv(feature_file_path, index=False)

    print(f"\nâœ… ç‰¹å¾æå–å®Œæˆ!")
    print(f"  ğŸ“ è¾“å‡ºç›®å½•: {extractor.output_dir}")
    print(f"  ğŸ“„ ç‰¹å¾æ–‡ä»¶: {feature_file_path}")
    print(f"  ğŸ‘¥ ç”¨æˆ·æ•°: {len(features_df):,}")
    print(f"  ğŸ”§ ç‰¹å¾æ•°: {len(features_df.columns)-1}")
    print(f"  ğŸ¯ é¢„æµ‹è´­ä¹°ç”¨æˆ·æ¯”ä¾‹: {features_df['likely_to_purchase'].mean():.3f}")

    # æ˜¾ç¤ºç‰¹å¾ç¤ºä¾‹
    print(f"\nğŸ“‹ ç‰¹å¾ç¤ºä¾‹ (å‰5ä¸ªç”¨æˆ·):")
    print(features_df[['user_id', 'total_actions', 'purchase_count', 'unique_items_count', 'likely_to_purchase']].head())

    print(f"\nğŸŠ ç‰¹å¾å·¥ç¨‹å®Œæˆï¼ä¸‹ä¸€æ­¥å¯ä»¥ç”¨è¿™äº›ç‰¹å¾è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹")
    print(f"ğŸ“ æ‰€æœ‰æ–‡ä»¶éƒ½ä¿å­˜åœ¨: {extractor.output_dir}")


if __name__ == "__main__":
    main()