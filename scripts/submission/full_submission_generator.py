#!/usr/bin/env python3
"""
å…¨é‡æ•°æ®submissionç”Ÿæˆå™¨
åªä½¿ç”¨æœ€é«˜æ•ˆçš„ç®—æ³•ï¼šæµè¡Œåº¦æ¨è
"""

import sys
import pandas as pd
import numpy as np
from collections import defaultdict
import time
from tqdm import tqdm

sys.path.append('src')

def load_full_data():
    """åŠ è½½å…¨é‡æ•°æ® - æ—¶é—´çª—å£ä¼˜åŒ–+åˆ†å—å¤„ç†"""
    print("ğŸ“‚ åŠ è½½å…¨é‡æ•°æ®ï¼ˆæ—¶é—´çª—å£ä¼˜åŒ–ï¼‰...")

    files = [
        "dataset/tianchi_fresh_comp_train_user_online_partA.txt",
        "dataset/tianchi_fresh_comp_train_user_online_partB.txt"
    ]

    columns = ["user_id", "item_id", "behavior_type", "user_geohash", "item_category", "time"]

    # æ—¶é—´çª—å£ï¼šé‡ç‚¹å…³æ³¨æœ€è¿‘7å¤© (2014-12-11 åˆ° 2014-12-17)
    target_dates = ["2014-12-11", "2014-12-12", "2014-12-13", "2014-12-14", "2014-12-15", "2014-12-16", "2014-12-17"]
    print(f"  ğŸ¯ æ—¶é—´çª—å£: {target_dates[0]} åˆ° {target_dates[-1]}")

    # åˆ†å—å¤§å°ï¼šæ¯æ¬¡å¤„ç†100ä¸‡è¡Œ
    chunk_size = 1000000
    all_chunks = []

    for file_path in tqdm(files, desc="ğŸ“ åŠ è½½æ–‡ä»¶"):
        print(f"  æ­£åœ¨åˆ†å—åŠ è½½ {file_path}")

        # åˆ†å—è¯»å–
        chunk_reader = pd.read_csv(file_path, sep="\t", names=columns, chunksize=chunk_size)

        file_chunks = []
        for chunk_num, chunk in enumerate(chunk_reader):
            # ç«‹å³è¿›è¡Œæ—¶é—´è¿‡æ»¤ä»¥å‡å°‘å†…å­˜
            chunk = chunk[chunk["behavior_type"].isin([1, 2, 3, 4])]
            chunk = chunk.dropna(subset=["user_id", "item_id", "time"])

            # æ—¶é—´è¿‡æ»¤ï¼šåªä¿ç•™ç›®æ ‡æ—¥æœŸ
            chunk = chunk[chunk["time"].str.startswith(tuple(target_dates))]

            if len(chunk) > 0:
                file_chunks.append(chunk)
                print(f"    å¤„ç†å— {chunk_num + 1}: {len(chunk):,} è¡Œ (æ—¶é—´è¿‡æ»¤å)")

        # åˆå¹¶å½“å‰æ–‡ä»¶çš„æ‰€æœ‰å—
        if file_chunks:
            file_df = pd.concat(file_chunks, ignore_index=True)
            all_chunks.append(file_df)
            print(f"  âœ“ {file_path}: {len(file_df):,} è¡Œ")

    print("ğŸ”„ åˆå¹¶æ‰€æœ‰æ•°æ®...")
    df = pd.concat(all_chunks, ignore_index=True)
    print(f"âœ… æ—¶é—´çª—å£å†…æ•°æ®é‡: {len(df):,} è¡Œ")

    return df

def preprocess_data(df):
    """åŸºç¡€æ•°æ®é¢„å¤„ç† - åŠ å…¥åœ°ç†å’Œç±»åˆ«ç‰¹å¾"""
    print("ğŸ§¹ æ•°æ®é¢„å¤„ç†...")

    print("  - è§£ææ—¶é—´...")
    df["datetime"] = pd.to_datetime(df["time"], format="%Y-%m-%d %H", errors="coerce")

    print("  - æ¸…ç†ç¼ºå¤±å€¼...")
    df = df.dropna(subset=["user_id", "item_id", "datetime"]).reset_index(drop=True)

    print("  - è½¬æ¢æ•°æ®ç±»å‹...")
    df["user_id"] = df["user_id"].astype(np.int32)
    df["item_id"] = df["item_id"].astype(np.int32)
    df["item_category"] = df["item_category"].astype("category")

    print("  - å¤„ç†åœ°ç†ä½ç½®ç‰¹å¾...")
    # å¤„ç†åœ°ç†ä½ç½®hash
    df["user_geohash"] = df["user_geohash"].fillna("unknown")
    df["geo_region"] = df["user_geohash"].astype("category")

    print("  - æ·»åŠ æ—¶é—´æƒé‡...")
    # åŸºäºæ—¥æœŸçš„æ—¶é—´è¡°å‡æƒé‡
    current_date = pd.to_datetime("2014-12-18")
    df["days_ago"] = (current_date - df["datetime"]).dt.days
    df["time_weight"] = np.exp(-0.2 * df["days_ago"])  # æ—¶é—´è¡°å‡å› å­

    print("  - æ·»åŠ è¡Œä¸ºæƒé‡...")
    # å¢å¼ºè¡Œä¸ºæƒé‡è€ƒè™‘åœ°ç†å› ç´ 
    behavior_weights = {1: 1.0, 2: 2.5, 3: 4.0, 4: 6.0}  # æé«˜é«˜ä»·å€¼è¡Œä¸ºæƒé‡
    df["behavior_weight"] = df["behavior_type"].map(behavior_weights)
    df["final_weight"] = df["behavior_weight"] * df["time_weight"]

    print(f"âœ… é¢„å¤„ç†å®Œæˆ: {len(df):,} è¡Œ")
    print(f"  ğŸ“ åœ°ç†ä½ç½®æ•°: {df['geo_region'].nunique()}")
    print(f"  ğŸ·ï¸  å•†å“ç±»åˆ«æ•°: {df['item_category'].nunique()}")

    return df

def build_popularity_model(df):
    """æ„å»ºå¢å¼ºæµè¡Œåº¦æ¨¡å‹ - èåˆåœ°ç†å’Œç±»åˆ«ç‰¹å¾"""
    print("ğŸ“Š æ„å»ºå¢å¼ºæµè¡Œåº¦æ¨¡å‹...")

    print("  - åŸºç¡€å•†å“æµè¡Œåº¦...")
    # ä½¿ç”¨é¢„å¤„ç†ä¸­è®¡ç®—çš„final_weight
    base_popularity = df.groupby("item_id")["final_weight"].sum()

    print("  - åœ°ç†åŠ æƒæµè¡Œåº¦...")
    # è®¡ç®—åœ°ç†ä½ç½®çš„è´­ä¹°åŠ›æƒé‡
    geo_purchase_power = df[df["behavior_type"] == 4].groupby("geo_region").size()
    geo_weights = (geo_purchase_power / geo_purchase_power.max()).fillna(0.1)  # æœ€å°æƒé‡0.1

    # ä¸ºæ¯ä¸ªå•†å“è®¡ç®—åœ°ç†åŠ æƒåˆ†æ•°
    geo_weighted_scores = {}
    for item_id in base_popularity.index:
        item_data = df[df["item_id"] == item_id]
        geo_score = 0
        for _, row in item_data.iterrows():
            geo_weight = geo_weights.get(row["geo_region"], 0.1)
            geo_score += row["final_weight"] * geo_weight
        geo_weighted_scores[item_id] = geo_score

    print("  - ç±»åˆ«çƒ­åº¦åŠ æƒ...")
    # è®¡ç®—ç±»åˆ«çƒ­åº¦
    category_popularity = df.groupby("item_category")["final_weight"].sum()
    category_weights = (category_popularity / category_popularity.max()).fillna(0.1)

    # å•†å“ç±»åˆ«æ˜ å°„
    item_category_map = df.groupby("item_id")["item_category"].first()

    print("  - èåˆå¤šç»´åº¦åˆ†æ•°...")
    final_scores = {}
    for item_id in base_popularity.index:
        base_score = base_popularity[item_id]
        geo_score = geo_weighted_scores.get(item_id, base_score * 0.1)
        category = item_category_map.get(item_id)
        category_weight = category_weights.get(category, 0.1) if category else 0.1

        # åŠ æƒèåˆï¼šåŸºç¡€0.5 + åœ°ç†0.3 + ç±»åˆ«0.2
        final_score = (0.5 * base_score +
                      0.3 * geo_score +
                      0.2 * base_score * category_weight)
        final_scores[item_id] = final_score

    print("  - æ’åºå•†å“...")
    popular_items = pd.Series(final_scores).sort_values(ascending=False)

    print(f"âœ… å¢å¼ºçƒ­é—¨å•†å“æ•°: {len(popular_items):,}")
    print(f"  ğŸ“ è€ƒè™‘ {df['geo_region'].nunique()} ä¸ªåœ°ç†åŒºåŸŸ")
    print(f"  ğŸ·ï¸  è€ƒè™‘ {df['item_category'].nunique()} ä¸ªå•†å“ç±»åˆ«")

    return popular_items

def get_user_history(df):
    """è·å–ç”¨æˆ·å†å²è¡Œä¸º - ä½¿ç”¨groupbyä¼˜åŒ–"""
    print("ğŸ“š æ„å»ºç”¨æˆ·å†å²...")

    print("  - èšåˆç”¨æˆ·è¡Œä¸ºæ•°æ®...")
    # ä½¿ç”¨groupbyæ›´é«˜æ•ˆ
    user_items = df.groupby("user_id")["item_id"].apply(set).to_dict()

    print(f"âœ… ç”¨æˆ·æ•°: {len(user_items):,}")
    return user_items

def generate_recommendations(user_items, popular_items, top_n=5):
    """ä¸ºç”¨æˆ·ç”Ÿæˆæ¨è"""
    print(f"ğŸ¯ ä¸º {len(user_items):,} ä¸ªç”¨æˆ·ç”Ÿæˆæ¨è...")

    # è·å–çƒ­é—¨å•†å“åˆ—è¡¨
    print("  - é€‰æ‹©å€™é€‰å•†å“...")
    top_popular = popular_items.head(200).index.tolist()  # å–å‰200çƒ­é—¨ä½œä¸ºå€™é€‰

    print("  - ç”Ÿæˆç”¨æˆ·æ¨è...")
    recommendations = {}

    for user_id, seen_items in tqdm(user_items.items(), desc="ğŸ¤– ç”Ÿæˆæ¨è"):
        # è¿‡æ»¤ç”¨æˆ·å·²è§å•†å“
        candidates = [item for item in top_popular if item not in seen_items]

        # å–å‰Nä¸ªæ¨è
        user_recs = candidates[:top_n]

        recommendations[user_id] = user_recs

    print(f"âœ… æ¨èç”Ÿæˆå®Œæˆ")
    return recommendations

def save_submission(recommendations, filename="enhanced_submission.txt"):
    """ä¿å­˜æäº¤æ–‡ä»¶"""
    print(f"ğŸ’¾ ä¿å­˜æäº¤æ–‡ä»¶: {filename}")

    total_lines = 0
    print("  - å†™å…¥æ–‡ä»¶...")

    with open(filename, "w", encoding="utf-8") as f:
        for user_id, items in tqdm(recommendations.items(), desc="ğŸ’½ å†™å…¥æ¨è"):
            for item_id in items:
                f.write(f"{user_id}\t{item_id}\n")
                total_lines += 1

    print(f"âœ… æäº¤æ–‡ä»¶å·²ç”Ÿæˆ")
    print(f"ğŸ“Š ç”¨æˆ·æ•°: {len(recommendations):,}")
    print(f"ğŸ“ æ¨èå¯¹æ•°: {total_lines:,}")
    return filename

def main():
    start_time = time.time()

    print("=== å…¨é‡æ•°æ®å¿«é€ŸSubmissionç”Ÿæˆå™¨ ===")
    print("ğŸš€ ä½¿ç”¨é«˜æ•ˆæµè¡Œåº¦ç®—æ³•")

    try:
        # 1. åŠ è½½æ•°æ®
        df = load_full_data()

        # 2. æ•°æ®é¢„å¤„ç†
        train_df = preprocess_data(df)

        # 3. æ„å»ºæµè¡Œåº¦æ¨¡å‹
        popular_items = build_popularity_model(train_df)

        # 4. è·å–ç”¨æˆ·å†å²
        user_items = get_user_history(train_df)

        # 5. ç”Ÿæˆæ¨è
        recommendations = generate_recommendations(user_items, popular_items, top_n=5)

        # 6. ä¿å­˜æäº¤æ–‡ä»¶
        submission_file = save_submission(recommendations)

        # ç»Ÿè®¡ä¿¡æ¯
        total_time = time.time() - start_time
        print(f"\nğŸ‰ å¤„ç†å®Œæˆ!")
        print(f"â±ï¸  æ€»ç”¨æ—¶: {total_time:.1f}ç§’")
        print(f"ğŸ¯ å¯ä»¥æäº¤ {submission_file} åˆ°æ¯”èµ›å¹³å°")

    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()