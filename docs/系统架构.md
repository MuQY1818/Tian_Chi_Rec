# 天池推荐系统架构文档

## 🎯 比赛目标
预测2014年12月19日用户对商品子集的购买行为，输出`user_id\titem_id`格式的submission.txt文件。

## 📊 数据规模
- **用户行为数据**: 11.65亿条记录 (50GB)
- **商品子集**: 678万商品
- **训练时间**: 2014.11.18-2014.12.18
- **预测目标**: 2014.12.19购买行为

## 🏗️ 系统架构

### 整体流程
```
原始数据 → 数据预处理 → 特征工程 → 模型训练 → 概率预测 → 结果输出
   ↓           ↓           ↓           ↓           ↓           ↓
  50GB      清洗过滤     57维特征     多模型      用户-商品   submission.txt
```

### 核心模块

#### 1. 数据预处理 (`src/data/large_scale_preprocessing.py`)
- **工具**: Dask分布式处理
- **功能**: 处理50GB原始数据，构建用户行为序列
- **输出**: 用户序列数据、统计特征

#### 2. 特征工程 (`src/features/feature_engineering.py`)
- **用户特征**: 行为频次、时间模式、偏好统计
- **商品特征**: 流行度、类别分布、时间衰减
- **交互特征**: 用户-商品交互强度、时间权重
- **总维度**: 57维特征

#### 3. 模型层 (`src/models/`)

##### GRU4Rec模型 (`deep_learning_models.py`)
- **用途**: 处理用户行为序列
- **输入**: 用户历史行为序列
- **输出**: 商品购买概率
- **特点**: GRU + 注意力机制

##### DeepFM模型 (`deep_learning_models.py`)
- **用途**: 特征交叉与非线性建模
- **输入**: 57维特征向量
- **输出**: 商品购买概率
- **特点**: FM + Deep学习

##### 基线模型 (`simple_baseline_models.py`)
- **用途**: 传统机器学习基准
- **模型**: 随机森林、逻辑回归
- **输出**: 商品购买概率

#### 4. 模型融合 (`src/models/model_ensemble.py`)
- **策略**: 加权平均 + Stacking
- **输入**: 多模型概率预测
- **输出**: 最终购买概率
- **优化**: 基于F1分数调整权重

## 🔧 关键技术

### 数据处理
- **分布式**: Dask处理50GB数据
- **分块**: 256MB数据块并行处理
- **内存优化**: 增量处理，避免OOM

### 特征工程
- **时间衰减**: `weight = exp(-0.1 * days_to_predict)`
- **行为权重**: 浏览(1)、收藏(2)、购物车(3)、购买(4)
- **序列构建**: 用户行为时间序列

### 模型训练
- **多GPU**: 支持GPU加速训练
- **负采样**: 1:3正负样本比例
- **早停**: 防止过拟合

## 📈 预期性能

### 模型性能
| 模型 | F1分数 | AUC | 训练时间 |
|------|--------|-----|----------|
| GRU4Rec | 0.75+ | 0.85+ | 2-4小时 |
| DeepFM | 0.78+ | 0.88+ | 1-3小时 |
| 集成模型 | 0.82+ | 0.90+ | 4-6小时 |

### 系统性能
- **数据处理**: 4-6小时 (50GB)
- **模型训练**: 4-6小时 (GPU)
- **推荐生成**: <30分钟
- **内存使用**: <32GB

## 🎯 输出格式

### 最终输出
```
user_id    item_id
1001      123456
1001      789012
1002      345678
...
```

### 输出要求
- **格式**: tab分隔，无表头
- **排序**: user_id升序
- **限制**: 每个用户最多推荐5个商品
- **商品**: 必须来自商品子集P

## 🚀 运行方式

### 完整流程
```bash
python main_large_scale.py --mode full --model_type ensemble
```

### 分步运行
```bash
# 数据预处理
python main_large_scale.py --mode preprocess

# 模型训练
python main_large_scale.py --mode train --model_type gru4rec

# 结果生成
python main_large_scale.py --mode predict --model_type ensemble
```

## 📁 文件结构

```
Tian_Chi_Rec/
├── main_large_scale.py              # 主程序
├── src/
│   ├── data/
│   │   ├── large_scale_preprocessing.py  # 大规模数据处理
│   │   └── simple_preprocessing.py      # 简单预处理
│   ├── features/
│   │   └── feature_engineering.py       # 特征工程
│   └── models/
│       ├── deep_learning_models.py      # 深度学习模型
│       ├── model_ensemble.py           # 模型融合
│       └── simple_baseline_models.py   # 基线模型
├── config/
│   ├── config.py                      # 配置文件
│   └── requirements.txt               # 依赖包
├── dataset/                          # 原始数据 (50GB)
├── data/processed/                   # 处理后数据
├── models/                          # 训练好的模型
├── logs/                           # 日志文件
└── submission.txt                   # 最终输出
```

## 🎯 核心创新点

1. **大规模处理**: Dask分布式处理50GB数据
2. **序列建模**: GRU4Rec捕捉用户行为序列
3. **特征交叉**: DeepFM学习高阶特征交互
4. **模型融合**: 多策略集成提升性能
5. **端到端**: 从原始数据到submission.txt全流程

## 📝 评估指标

- **主要指标**: F1分数 (比赛唯一标准)
- **辅助指标**: Precision, Recall, AUC
- **优化目标**: 最大化F1分数

这个架构专注于比赛的核心需求，通过大规模数据处理和深度学习模型，生成高质量的购买概率预测。